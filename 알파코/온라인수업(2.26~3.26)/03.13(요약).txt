<03.13>

## 58차시 : 인경신경망과 딥러닝 만들기------------------------------------------------------------------------------------------

    # 은닉층이 존재하면 딥러닝

    ## <인공신경망>
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from tensorflow import keras
from sklearn.preprocessing import StandardScaler

    # 1) 데이터 가져오기
iris = load_iris()
data,target = iris['data'],iris['target']

    # 2) 데이터 분할
train_input,test_input,train_target,test_target = train_test_split(data,target)

    # 3) 데이터 표준화
ss = StandardScaler()
train_scaled = ss.fit_transform(train_input)
test_scaled = ss.transform(test_input)

    # 4) 모델 생성
model = keras.Sequential() #인공신경망을 만들기 위한 도화지
        
        # iris 데이터에 4개의 특성이 있고 그 특성이 3개의 노드(출력층)로 향함
model.add(keras.layers.Dense(3,activation='softmax',input_shape=(4,)))   
        
        # 모델이 어떡해 학습할지 설정
model.compile(loss='sparse_categorical_crossentropy',metrics = ['accuracy'])
        
        # 모델 사용
hist = model.fit(train_scaled,train_target,epochs = 500)

    # 5) loss값의 추이 그래프로 확인
import matplotlib.pyplot as plt
plt.plot(hist.history['loss'])

    # 6) 모델 평가
model.evaluate(test_scaled,test_target)


    ## <딥러닝>
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from tensorflow import keras
from sklearn.preprocessing import StandardScaler

    # 1) 데이터 가져오기
iris = load_iris()
data,target = iris['data'],iris['target']
    
    # 2) 데이터 분할
train_input,test_input,train_target,test_target = train_test_split(data,target)

    # 3) 데이터 표준화
ss = StandardScaler()
train_scaled = ss.fit_transform(train_input)
test_scaled = ss.transform(test_input)

    # 4) 모델 생성
model = keras.Sequential() 
model.add(keras.layers.Dense(8,activation='relu',input_shape=(4,)))       # 은닉층
model.add(keras.layers.Dense(3,activation='softmax'))                     # 출력층
        
        # 모델이 어떡해 학습할지 설정
model.compile(loss='sparse_categorical_crossentropy',metrics = ['accuracy'])
        
        # 모델 사용
hist = model.fit(train_scaled,train_target,epochs = 500)

    # 5) loss값의 추이 그래프로 확인
import matplotlib.pyplot as plt
plt.plot(hist.history['loss'])

    # 6) 모델 평가
model.evaluate(test_scaled,test_target)


## 59차시 : 최적의 딥러닝 모델 만들기-------------------------------------------------------------------------------------------------------------------

    # <최적화 옵션>
    # EarlyStopping : 과대적합이 일어날 때 자동으로 학습 종료
    # ModelCheckPoint : 최상의 모델이 발견될 때마다 자동 저장
    # Dropout : 인공신경망이 완벽하게 학습하지 못하도록 연산 방해

from tensorflow import keras
from sklearn.model_selection import train_test_split

    # 1) 이미지(옷) 데이터
(train_input,train_target),(test_input,test_target) = keras.datasets.fashion_mnist.load_data()

    # 2) 데이터 모양 확인
test_input.shape      # 1만개 (3차원)
train_input.shape     # 6만개 (3차원)

    # 3) 검증 데이터 만들기 
train_input,val_input,train_target,val_target = train_test_split(train_input,train_target,test_size=0.1)  # 학습데이터(9):검증데이터(1) 비율로 할당

    # 4) 데이터 그림으로 확인
# import matplotlib.pyplot as plt
# plt.imshow(train_input[1])

    # 5) 데이터 모양 변형 : 3차원 -> 2차원  /  표준화
        # 픽셀값 0~255 : 0에 가까울수록 검은색 반대가 흰색 / 0 과 255 사이 차이가 크므로 전체를 255로 나눠 0과1사이로 바꾼다
train_scaled = train_input.reshape(-1,784) / 255
val_scaled = val_input.reshape(-1,784) / 255
test_scaled = test_input.reshape(-1,784) / 255

train_scaled.shape
val_scaled.shape
test_scaled.shape

    # 6) 모델 만들기
        # 두개의 은닉층
model = keras.Sequential()
model.add(keras.layers.Dense(256,activation='relu',input_shape=(784,)))
model.add(keras.layers.Dropout(0.3))                  # 과대적합 막기위해 : 이 은닉층의 30%는 랜덤으로 끄겠다
model.add(keras.layers.Dense(128,activation='relu'))
model.add(keras.layers.Dropout(0.3))                  # 과대적합 막기위해 : 이 은닉층의 30%는 랜덤으로 끄겠다
model.add(keras.layers.Dense(10,activation='softmax'))

    # 7) 모델 어떡해 학습할지
model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer="adam") # 학습률 조정

        # 10) 과대적합을 막아보자
                # 5번 연속 나빠지면 학습 강제종료, 마지막 5번은 삭제한다.
es = keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)
cp = keras.callbacks.ModelCheckpoint('best.keras')

    # 8) 모델 학습
        # 주의 깊게 볼 것은 val_loss 즉) 검증 데이터의 오차값 (쪽지시험 결과값이라고 생각) -> 검증 데이터 오차값이 오르면 과대적합이 된거임 / 너무 많은 학습을 해서 학습데이터에만 적합해짐
hist = model.fit(train_scaled,train_target,epochs=30,validation_data = (val_scaled,val_target),
                 callbacks=[es,cp])

    # 9) 오차값 / 검증데이터의 오차값 그림 확인
import matplotlib.pyplot as plt
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.legend(hist.history['loss'],hist.history['val_loss'])


## 60차시 : 이미지를 위한 딥러닝 CNN----------------------------------------------------------------------------------------------------------------------------------------------------------------------------


    # 59차시 모델로 학습해도 되지만 CNN으로 이미지를 분석하는것이 더 좋음

from tensorflow import keras
from sklearn.model_selection import train_test_split

    # 1) 이미지(옷) 데이터
(train_input,train_target),(test_input,test_target) = keras.datasets.fashion_mnist.load_data()

    # 2) 검증 데이터 만들기 
train_input,val_input,train_target,val_target = train_test_split(train_input,train_target,test_size=0.1)  # 학습데이터(9):검증데이터(1) 비율로 할당

    # 3) 데이터 모양 변형 : CNN에 넣을 때는 4차원으로 만들어야 함  /  표준화
train_scaled = train_input.reshape(-1,28,28,1) / 255
val_scaled = val_input.reshape(-1,28,28,1) / 255
test_scaled = test_input.reshape(-1,28,28,1) / 255

    # 4) 모델 만들기
        # 두개의 은닉층
model = keras.Sequential()
model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu',input_shape=(28,28,1)))
model.add(keras.layers.MaxPooling2D(2))   # 반으로 줄임
model.add(keras.layers.Conv2D(filters=16,kernel_size=(3,3),padding='same',activation='relu'))
model.add(keras.layers.MaxPooling2D(2))

model.add(keras.layers.Flatten())                       # 모델을 쭉 핀다
model.add(keras.layers.Dense(100,activation='relu'))    # 은닉층으로 보내기
model.add(keras.layers.Dropout(0.3)) 
model.add(keras.layers.Dense(10,activation='softmax'))


        # 8) 과대적합을 막아보자
es = keras.callbacks.EarlyStopping(patience=15,restore_best_weights=True)

    # 5) 모델 어떡해 학습할지
model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer="adam") # 학습률 조정

    # 6) 모델 학습
hist = model.fit(train_scaled,train_target,epochs=1000,validation_data = (val_scaled,val_target),
                 callbacks=[es])

    # 7) 성능 평가
model.evaluate(test_scaled,test_target)


## 61차시 : 시계열 데이터를 위한 딥러닝 RNN / LSTM---------------------------------------------------------------------------------------------------------------------------------------


    # 전에 들어왔던 데이터를 기억하게 함으로써 다음 값이 전 데이터 값이 
    # 어떤 값인지 인지하게 한다.

from tensorflow import keras
from sklearn.model_selection import train_test_split

    # 1) 영화 리뷰 데이터(긍정, 부정) // 많이 사용된 순으로 1~500위 까지 가져온다
(train_input,train_target),(test_input,test_target) = keras.datasets.imdb.load_data(num_words = 300)

    # 2) 검증 데이터 만들기 
train_input,val_input,train_target,val_target = train_test_split(train_input,train_target,test_size=0.2)  # 학습데이터(8):검증데이터(2) 비율로 할당

        # 입력층에 들어갈 데이터들의 모양을 같게 만들어 줘야 함
    # 3) 사람들이 리뷰에 평균적으로 몇글자 쓰나
import numpy as np
box = []
for i in train_input:
    box.append(len(i))

np.mean(box)

    # 4) 단어에 개수를 맞추자 // 단어가 적든 많든 무조건 리뷰 안에 있는 단어를 100개로 맞춘다
from keras.preprocessing.sequence import pad_sequences
train_seq = pad_sequences(train_input,maxlen=100)
val_seq = pad_sequences(val_input,maxlen=100)
        # 단어가 많으면 앞에서부터 자른다
        # 단어가 적으면 0으로 채워서라도 100개를 맞춘다

    # 5) 데이터 전처리 / 컴퓨터가 어느 한 단어에 치우치지 않게 모든 단어들을 평등하게 바꾼다
train_seq[1]
        # 원핫인코딩 : 해당 숫자만 1이고 나머지는 0
train_oh = keras.utils.to_categorical(train_seq)
val_oh = keras.utils.to_categorical(val_seq)
train_oh[1]

    # 6) 모델 만들기
model = keras.Sequential()
model.add(keras.layers.SimpleRNN(8,input_shape=(100,300)))
model.add(keras.layers.Dense(2,activation="softmax"))

    # 7) 모델 어떡해 학습할지
model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer="adam")   # 학습률 조정

        # 8) 과대적합을 막아보자
es = keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)

    # 8) 모델 학습
his = model.fit(train_oh,train_target,epochs=100,validation_data = (val_oh,val_target),callbacks=[es])

## 62차시 : 데이터 크롤링을 통한 삼성전자 주식 예측하기----------------------------------------------------------------------------------------------------------------------------------------

    # 1) 데이터 크롤링
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
from tqdm import tqdm

        # 우회
dic = {'User-Agent':'Mozilla/5.0(Macintosh; Intel Mac OS X 12_6)'}

        # 첫페이지 부터 끝페이지까지 가져오기
total = []
for i in tqdm(range(1,660)):
    url = requests.get("https://finance.naver.com/item/sise_day.naver?code=005930&page={}".format(i),headers=dic)
    html = BeautifulSoup(url.text)

    table = html.find('table')
    table = pd.read_html(str(table))[0].dropna()
    del table['전일비']
    
    total.append(table)                                    

    time.sleep(0.1)

len(total)

    # 2) 데이터 전처리
        # 하나의 데이터프레임으로 만들기
samsung = pd.concat(total,ignore_index=True)
samsung

        # 날짜 타입 변환
samsung['날짜'] = pd.to_datetime(samsung['날짜'])
samsung.info()

        # 과거에서 현재로 바꾸기
samsung = samsung[::-1]
samsung

        # 인덱스 재설정
samsung.index = range(len(samsung))
samsung

    # 3) 문제집/정답지 만들기
data = []
target = []
for i in range(len(samsung)-1):
    a = list(samsung.iloc[i,1:])
    b = samsung.iloc[i+1,1]
    data.append(a)
    target.append(b)

        # 행렬로 변환
import numpy as np
data = np.array(data)
target = np.array(target)

data.shape                          # 2차원
target.shape                        # 1차원


    # 4) 모델 설정
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(data,target)

a = list(samsung.iloc[-1][1:])      # 3/13일 주가
rf.predict([a])                     # 3/14일 주가 예측 73,601원