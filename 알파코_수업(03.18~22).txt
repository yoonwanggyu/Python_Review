## 03.18 수업------------------------------------------------------------------------------------------------------------------------------------------
    # 웹 크롤링 복습
    # 원하는 태그들만의 공통점 찾기 -> 속성 + 계층구조(상위태그 + 특성)

    # 크롤링시, 인덱싱, 슬라이싱을 select와 함께 적극적으로 활용

    # 섭네일 사진 가져오기
    # 책 정보 html 가져오기 -> 해당 정보 태그 가져오기 -> 정보에 있는 url requests로 다시 불러오기 -> 불러운 url 저장

    # 1) 정보와 똑같이 접속
url = requests.get("https://www.aladin.co.kr/shop/common/wbest.aspx?BestType=Bestseller&BranchType=1&CID=170")
soup = BeautifulSoup(url.content,"lxml")

    # 2) 이미지 태그 찾기
img_tag = soup.select("img.front_cover")[0]
print(img_tag)

    # 3) 이미지 태그에서 'src' 만 가져오기
img_src = img_tag['src']

    # 4) 'src'에 있는 url으로 접속해서 requests.get 적용
img_resp = requests.get(img_src)
img_resp.content

    # 5) 파일에 저장
with open('불변의 법칙.jpg','wb') as f:
    f.write(img_resp.content)


    ## iframe은 파싱이 안된다
    ## 해당 주소 들어가서 파싱해야 된다

    # 1) 
url = 'https://finance.naver.com/sise/sise_index_day.naver?code=KOSPI'
resp = requests.get(url)
    # 2)
soup = BeautifulSoup(resp.content,'lxml')
    # 3)
date = [date.text for date in soup.select("td.date")]
price = [price.text for price in soup.select("td.number_1")[::2]][::2]
diff = [('-' + diff.text.strip() if 'red02' not in diff['class'] else diff.text.strip()) for diff in soup.select("td.rate_down span")]
rate = [rate.text.strip() for rate in soup.select("td.number_1 span.tah")]
volumn = [volumn.text.strip() for volumn in soup.select("td.number_1")[2::4]]
amount = [amount.text.strip() for amount in soup.select("td.number_1")[3::4]]
    #4)
df_ksp200 = pd.DataFrame({"날짜":date,
                          "체결가" : price,
                          "전일비": diff,
                          "등락률" : rate,
                          "거래량" : volumn,
                          "거래대금" : amount})
df_ksp200


## 03.19 수업-----------------------------------------------------------------------------------------------------------------------

    # 요청이 거절당했을 때 -> headers를 바꿔서 접근해라
    # 언론사별로 형식이 다르므로 '네이버뉴스' 통합 페이지 href만 보아서 다시 접근

    # javascript 1) disable js 하고 -> 강아지 사진 가져오기
url = requests.get("https://www.google.com/search?q=%EA%B0%95%EC%95%84%EC%A7%80&sca_esv=074560fb4084e5a7&biw=1536&bih=826&gbv=1&tbm=isch&ei=xSz5ZaztCIyk2roPv_yU-Ak&start=0&sa=N")
soup = BeautifulSoup(url.content,"lxml")

img_tags = soup.select("img.DS1iW")
len(img_tags)

for idx,img_tag in enumerate(img_tags):
    img = requests.get(img_tag['src'])

    with open(f"img_{idx}.jpg",'wb') as f:
        f.write(img.content)


    # 크롤링 할 때의 루틴 !!
        
        # 1) 설정 들어가서 disable JS 하기
            # 2) 자바로 바뀐 형태로 페이지가 뜨면 기본 루틴으로
            # 3) 페이지가 뜨지 않는다 -> url 찾기 -> network -> fetch/js -> preview -> General request url 복사
        # 2) iframe 확인
            # 3) 맞다면 태그로 가서 src 에서 기본 루틴으로
        # 3) 앞 상황이 아니라면 기본 루틴 -> 요청 거절이면 headers 바꾸기 


    # javascript 2)
        # 1. 자바 스크립트 페이지 url 찾아서 페이지 가져오기
import requests
import json
from tqdm.notebook import tqdm

url = "https://www.bluer.co.kr/api/v1/restaurants?page=0&size=30&query=&foodType=&foodTypeDetail=&feature=&location=&locationDetail=&area=&areaDetail=&priceRange=&ribbonType=&recommended=false&isSearchName=false&tabMode=single&searchMode=ribbonType&zone1=&zone2=&zone2Lat=&zone2Lng="
resp = requests.get(url)
resp.text
        # 2. json을 딕셔너리로 형변환하기
jsn = json.loads(resp.content)
jsn["_embedded"]["restaurants"]

        # 3. 필요한 정보 가져와서 데이터프레임으로 저장하기

name, ribbon, year = [], [], []
restaurants = jsn["_embedded"]["restaurants"]

for restaurant in restaurants:
    name.append(restaurant["headerInfo"]["nameKR"])
    ribbon.append(restaurant["headerInfo"]["ribbonType"])
    year.append(restaurant["headerInfo"]["year"])

name,ribbon,year

        # 4. 그럼 위의 코드를 바탕으로 모든 레스토랑의 아래 항목을 모두 파싱하여 csv로 저장해보세요.
tels, dayoffs, parks, prices, menus, times = [], [], [], [], [], []
restaurants = jsn["_embedded"]["restaurants"]

for restaurant in restaurants:
    menus.append(restaurant["statusInfo"]["menu"])
    prices.append(restaurant["statusInfo"]["priceRange"])
    parks.append(restaurant["statusInfo"]["parking"])
    dayoffs.append(restaurant["defaultInfo"]["dayOff"])
    tels.append(restaurant["defaultInfo"]["phone"])
    times.append(restaurant["statusInfo"]["businessHours"])

tels, dayoffs, parks, prices, menus, times

## 03.20 수업-----------------------------------------------------------------------------------------------------------------------

    # 데이터 분석

    # 데이터를 보는 통찰력을 기르자!!
    # 하나의 문제를 끝까지 생각할 수 있는 힘

    # 머신러닝은 cost가 낮은 hypothesis를 찾는 것이다!!!    

    # 머신런닝은?
    #-> cost가 가장 낮을 때의 weight를 찾는 과정!!

import requests
import json
from tqdm.notebook import tqdm
from bs4 import BeautifulSoup

    # 실습 : 한 웹툰 전체 화 전체 댓글 가져오기
box = []
for i in tqdm(range(1,51)):
    url = 'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?'


    params = {'ticket': 'comic',
              'templateId': 'webtoon',
              'pool' : 'cbox3',
              '_cv' : '20240319154009',
              '_callback' : '',
              'lang': 'ko',
              'country' : 'KR',
              'objectId' : f'808198_{i}',
              'pageSize' : '15',
              'indexSize' : '10',
              'groupId' : '808198',
              'listType' : 'OBJECT',
              'pageType' : 'more',
              'page' : '1',
              'currentPage' : '1',
              'refresh' : 'true',
              'sort' : 'new',
              '_': 1710914339145}

    headers = {"referer" : f"https://comic.naver.com/webtoon/detail?titleId=808198&no=50&week=thu",
           "user-agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}
    respond = requests.get(url,params=params, headers=headers)


    # print(len('jQuery3630057019423799983926_1710895256081('))
    # print(respond.text)

    jsn2 = respond.text.replace("_callback(","")[:-2]


    dic2 = json.loads(jsn2)

    total_pages = dic2['result']['pageModel']['totalPages']

    # print(total_pages)

    for j in range(1,total_pages+1):
        url = f'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=comic&templateId=webtoon&pool=cbox3&_cv=20240319154009&_callback=jQuery3630057019423799983926_1710895256081&lang=ko&country=KR&objectId=808198_{i}&categoryId=&pageSize=15&indexSize=10&groupId=808198&listType=OBJECT&pageType=more&page={j}&currentPage={j}&refresh=true&sort=new&_=1710895256093'
        headers = {"referer" : "https://comic.naver.com/webtoon/detail?titleId=808198&no=50&week=thu",
            "user-agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}
        resp = requests.get(url, headers=headers)
        jsn = resp.text.replace("jQuery3630057019423799983926_1710895256081(","").replace(");","")
        dic = json.loads(jsn)

        comments = dic['result']['commentList']

        for comment in comments:
            box.append(comment["contents"])

## 03.21 수업-----------------------------------------------------------------------------------------------------------------------
            
    ## 텍스트 마이닝 -> NLP
    
    ## 패키지 : NLTK / SpaCy
            
    ## 토큰(token)이란 문법적으로 더 이상 나눌 수 없는 언어요소를 뜻함. 텍스트 토큰화(Text Toeknization)란 말뭉치로부터 토큰을 분리하는 작업을 뜻함.
            
    ## 토큰화(Tokenization) : 주어진 텍스트를 원하는 단위(토큰, token)로 나누는 작업을 말함. 
    ## 원하는 단위가 문장인 경우에는 문장 토큰화(Sentence tokenization)라고 하고, 
    ## 단어인 경우에는 단어 토큰화(word tokenization)라고 한다.
            

    # 1) -----spaCy--------------------------------------------
    ## #spaCy는 토큰화에 필요한 데이터를 다운로드 해야함.
    ## python -m spacy download en_core_web_sm
            
import spacy
spacy_en = spacy.load("en_core_web_sm")
text = "David's book wasn't famous, but his family loved his book."
for token in spacy_en(text):
    print(token)

        # spaCy를 이용해 토큰화를 수행하면 기본적으로 토큰외에도 PoS(품사), lemma등의 정보를 알 수 있다.
for token in spacy_en.tokenizer(text):
    print(f"token: {token.text}, PoS: {token.pos_}, lemman: {token.lemma_}")

        # 불용어
stop_words = spacy.lang.en.stop_words.STOP_WORDS   # 자체 불용어 딕셔너리
for i, stop_word in enumerate(stop_words):
    if i == 10:
        break
    print(stop_word)

    # 2) -----NLTK----------------------------------------------
    # NLTK의 Tokenizer(토크나이저)를 사용하기 위해서는 데이터(NLTK Data)를 설치해야한다.

import nltk
nltk.download("popular")

        # 2-1) sentence_tokenizer (점 + 공백 두개를 합친것을 기준으로)
from nltk.tokenize import sent_tokenize
sent_tokenize("I like you. You're my son.")

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))

        # 2-2) word_tokenizer
        # 다만, 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 함.
        # 단어 토큰화는 기본적으로 띄어쓰기를 기준으로 함.
        # 영어보는 보통 띄어쓰기로 토큰이 구분되는 반면, 한국어는 띄어쓰기 만으로 토큰을 구분하기 어려움.

from nltk.tokenize import word_tokenize
nltk.download('punkt')
word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.")
word_tokenize("David's book wasn't famous, but his family loved his book.")

text = input("Please write a text:")
print("The length of the text is",len(word_tokenize(text)),"words")

        # (') 어퍼스트로피가 있는 경우 어떻게 구분할까? -> WordPunctTokenizer = word_tokenize와는 달리 '(어퍼스트로피)를 별도의 토큰으로 구분해서 토큰화를 진행한다.
from nltk.tokenize import WordPunctTokenizer
text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
words = WordPunctTokenizer().tokenize(text)
words

        # (') 어퍼스트로피가 있는 경우 어떻게 구분할까? -> text_to_word_sequence = 모든 알파벳을 소문자로 바꾸고, 구두점(컴마, 마침표 등)을 없애고, 어퍼스트로피(')도 보존하여 토큰을 제대로 구분해줍니다.
from tensorflow.keras.preprocessing.text import text_to_word_sequence
text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
words = text_to_word_sequence(text)
words

    # 3) -----Penn Treebank Tokenization---------------------------------------------
    # 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.
    # 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.

from nltk.tokenize import TreebankWordTokenizer
text = "Starting a home-based restaurant may be an ideal. It doesn't have a food chain or restaurant of their own."
words = TreebankWordTokenizer().tokenize(text)
words


## 03.22 수업-----------------------------------------------------------------------------------------------------------------------

    # 1) TweetTokenizer.tokenize-------------##
    # 이모티콘을 인식해주는 토큰화를 지원함
from nltk.tokenize import TweetTokenizer
tweet = "Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)"
tokenizer = TweetTokenizer()
tokenizer.tokenize(tweet)

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
print(word_tokenize("Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)"))

    # 2) kss.split_sentences-------------------##
    # 한국어에 대한 문장 토큰화 도구 또한 존재

import kss
text = '딥러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'
kss.split_sentences(text)

    # 3) N-gram-----------------------------------##
    # n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석
    # n=1일 때는 unigramm, 2일 대는 bigram, 3일때는 trigram

from nltk import ngrams
sentence = 'There is no royal road to learning'
list(ngrams(sentence.split(),1))
list(ngrams(sentence.split(),2))
list(ngrams(sentence.split(),3))

cleaned = " mary , n't slap green witch' , "
list(ngrams(cleaned.split(),3))

def n_grams(text,n):
    return [text[i:i+n] for i in range(len(text)-n+1)]
clean = ["mary",",","n't","slap","green",'witch',"."]
n_grams(clean,3)


    # 4) 불용어 제거----------------------------------##
    # 불용어(Stop word)는 분석에 큰 의미가 없는 단어를 지칭

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

print('영어 불용어 갯수:',len(nltk.corpus.stopwords.words('english')))
print(nltk.corpus.stopwords.words('english')[:40])

example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english'))                    # set() 함수로 중복 제거
word_tokens = word_tokenize(example)

result = []
for token in word_tokens:
    if token not in stop_words:
        result.append(token)

print(word_tokens)   # 문장에서 전체 단어
print(result)        # 문장에서 불용어 제외한 단어만



stop_words = "is not an it's thing"                 # 직접 불용어 사전 만들기
stop_words = text_to_word_sequence(stop_words)      # (')어퍼스트로피도 구분해주는 text_to_word_sequence 사용

text = "Family is not an important thing. It's everything."
text = text_to_word_sequence(text)

nouns = []
for noun in text:
    if noun not in stop_words:
        nouns.append(noun)

nouns

        # 텍스트 전처리의 목적은 말뭉치(Corpus)로부터 복잡성을 줄이는 것입니다

    # 5) PorterStemmer-------------------------------------------##
    # 어간 추출 : 단어의 의미를 담고 있는 단어의 핵심 부분

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))
print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))
print(stemmer.stem('happier'),stemmer.stem('happiest'))
print(stemmer.stem('fancier'),stemmer.stem('fanciest'))
print(stemmer.stem('was'), stemmer.stem('love'))

    # 6) Lancaster Stemmer---------------------------------------------##
from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()

print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))
print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))
print(stemmer.stem('happier'),stemmer.stem('happiest'))
print(stemmer.stem('fancier'),stemmer.stem('fanciest'))
print(stemmer.stem('was'), stemmer.stem('love'))

    # 7) RegexpStemmer class-----------------------------------------------##
    # 사용자가 지정한 정규표현을 기준으로 동작함
from nltk.stem.regexp import RegexpStemmer
stemmer = RegexpStemmer('ing')
print(stemmer.stem('cooking'))
print(stemmer.stem('cookery'))
print(stemmer.stem('ingleside'))

    # 8) SnowballStemmer class------------------------------------------------##
from nltk.stem.snowball import SnowballStemmer
spanish_stemmer = SnowballStemmer('english')
print(spanish_stemmer.stem('cooking'))


    # 9) 표제어 추출--------------------------------------------------------------##

from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

lemma = WordNetLemmatizer()
print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))
print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))
print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))
print(lemma.lemmatize('was', 'v'), lemma.lemmatize('love', 'v'))
print(lemma.lemmatize('dies','v'))
print(lemma.lemmatize('watched', 'v'))
print(lemma.lemmatize('has', 'v'))

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
verbs = []
for w in words:
    lemma = WordNetLemmatizer().lemmatize(w,'v')
    if lemma != w:
        verbs.append(lemma)

verbs


    # 10) 정규표현식
import re

        # 1) .은 한 개의 임의의 문자를 나타낸다. 예를 들어 정규 표현식이 a.c라고 한다면, a와 c사이에 어떤 1개의 문자라도 올 수 있다는 뜻이다.
r = re.compile("a.c")
r.search("abc")
r.search("ac")

        # 2) ?는 ? 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있는 경우를 나타낸다. 
r = re.compile("ab?c")
r.search("ac")
r.search("abcd")

        # 3) *은 바로 앞의 문자가 0개 이상일 경우를 나타낸다. 앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수도 있다.
r = re.compile("ab*c")
r.search("abbc")
r.search("ac")

        # 4) +는 *와 유사하다. 하지만 다른 점은 앞의 문자가 최소 1개 이상이어야 한다.
r = re.compile("ab+c")
r.search("ac")
r.search("abbc")

        # 5) ^는 시작되는 글자를 지정한다. 가령 정규표현식이 ^a라면 a로 시작되는 문자열만을 찾아낸다.
r = re.compile("^c")
r.search("coomputer")
r.search("apple")

        # 6) {숫자}기호 문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타낸다.
r = re.compile("ab{2}c")  
r.search("abbc")

r = re.compile("ab{1,3}c")  # b가 1개~3개
r.search("abc")
r.search("abbbc")

r = re.compile("ab{2,}c")   # b가 2개 이상
r.search("abbc")
r.search("abbbbbbc")

        # 7) []안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치라는 의미를 가진다
r = re.compile("[abc]")
r.search('acg')
r.search('babo')
r.search('cdef')

        # 8) [^문자]는 5)에서 설명한 ^와는 완전히 다른 의미로 쓰인다. 여기서는 ^ 기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할을 한다.
r = re.compile('[^abc]')
r.search("1st")

        # 9) (1) re.match()와 re.search()의 차이
        #    search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규표현식과 매치하는지를 확인한다.

r = re.compile("ab.")
r.search("abc")
r.match("ffabc")

        # 10) split() 함수는 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴한다.
text = "사과+딸기+수박+멜론+바나나"
print(re.split("\+",text))
print(text.split("+"))

        # 11) 3) re.findall() 함수는 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴한다.
text = """이름 : 김철수
전화번호 : 010 - 1234 - 1234
나이 : 30
성별 : 남"""
pattern = "[0-9]+"
a = re.findall(pattern,text)   # re.findall("\d+",text)
print(a)

        # 12) re.sub() 함수는 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있다.
text="Regular expression : regex or regexp[1] (sometimes called a rational expression)[2][3] is, a sequence of characters that define a search pattern."
re.sub('[^a-zA-Z]',' ',text)    # 영어 제외한 나머지 모두 공백 처리
text="Regular expression : regex or regexp[1]  안녕 (sometimes called a rational expression)[2][3] is, 하세요, a sequence of characters that define a search pattern."
re.sub('[^가-힣]',' ', text)



