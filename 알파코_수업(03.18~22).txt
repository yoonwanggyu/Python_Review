## 03.18 수업------------------------------------------------------------------------------------------------------------------------------------------
    # 웹 크롤링 복습
    # 원하는 태그들만의 공통점 찾기 -> 속성 + 계층구조(상위태그 + 특성)

    # 크롤링시, 인덱싱, 슬라이싱을 select와 함께 적극적으로 활용

    # 섭네일 사진 가져오기
    # 책 정보 html 가져오기 -> 해당 정보 태그 가져오기 -> 정보에 있는 url requests로 다시 불러오기 -> 불러운 url 저장

    # 1) 정보와 똑같이 접속
url = requests.get("https://www.aladin.co.kr/shop/common/wbest.aspx?BestType=Bestseller&BranchType=1&CID=170")
soup = BeautifulSoup(url.content,"lxml")

    # 2) 이미지 태그 찾기
img_tag = soup.select("img.front_cover")[0]
print(img_tag)

    # 3) 이미지 태그에서 'src' 만 가져오기
img_src = img_tag['src']

    # 4) 'src'에 있는 url으로 접속해서 requests.get 적용
img_resp = requests.get(img_src)
img_resp.content

    # 5) 파일에 저장
with open('불변의 법칙.jpg','wb') as f:
    f.write(img_resp.content)


    ## iframe은 파싱이 안된다
    ## 해당 주소 들어가서 파싱해야 된다

    # 1) 
url = 'https://finance.naver.com/sise/sise_index_day.naver?code=KOSPI'
resp = requests.get(url)
    # 2)
soup = BeautifulSoup(resp.content,'lxml')
    # 3)
date = [date.text for date in soup.select("td.date")]
price = [price.text for price in soup.select("td.number_1")[::2]][::2]
diff = [('-' + diff.text.strip() if 'red02' not in diff['class'] else diff.text.strip()) for diff in soup.select("td.rate_down span")]
rate = [rate.text.strip() for rate in soup.select("td.number_1 span.tah")]
volumn = [volumn.text.strip() for volumn in soup.select("td.number_1")[2::4]]
amount = [amount.text.strip() for amount in soup.select("td.number_1")[3::4]]
    #4)
df_ksp200 = pd.DataFrame({"날짜":date,
                          "체결가" : price,
                          "전일비": diff,
                          "등락률" : rate,
                          "거래량" : volumn,
                          "거래대금" : amount})
df_ksp200


## 03.19 수업-----------------------------------------------------------------------------------------------------------------------

    # 요청이 거절당했을 때 -> headers를 바꿔서 접근해라
    # 언론사별로 형식이 다르므로 '네이버뉴스' 통합 페이지 href만 보아서 다시 접근

    # javascript 1) disable js 하고 -> 강아지 사진 가져오기
url = requests.get("https://www.google.com/search?q=%EA%B0%95%EC%95%84%EC%A7%80&sca_esv=074560fb4084e5a7&biw=1536&bih=826&gbv=1&tbm=isch&ei=xSz5ZaztCIyk2roPv_yU-Ak&start=0&sa=N")
soup = BeautifulSoup(url.content,"lxml")

img_tags = soup.select("img.DS1iW")
len(img_tags)

for idx,img_tag in enumerate(img_tags):
    img = requests.get(img_tag['src'])

    with open(f"img_{idx}.jpg",'wb') as f:
        f.write(img.content)


    # 크롤링 할 때의 루틴 !!
        
        # 1) 설정 들어가서 disable JS 하기
            # 2) 자바로 바뀐 형태로 페이지가 뜨면 기본 루틴으로
            # 3) 페이지가 뜨지 않는다 -> url 찾기 -> network -> fetch/js -> preview -> General request url 복사
        # 2) iframe 확인
            # 3) 맞다면 태그로 가서 src 에서 기본 루틴으로
        # 3) 앞 상황이 아니라면 기본 루틴 -> 요청 거절이면 headers 바꾸기 


    # javascript 2)
        # 1. 자바 스크립트 페이지 url 찾아서 페이지 가져오기
import requests
import json
from tqdm.notebook import tqdm

url = "https://www.bluer.co.kr/api/v1/restaurants?page=0&size=30&query=&foodType=&foodTypeDetail=&feature=&location=&locationDetail=&area=&areaDetail=&priceRange=&ribbonType=&recommended=false&isSearchName=false&tabMode=single&searchMode=ribbonType&zone1=&zone2=&zone2Lat=&zone2Lng="
resp = requests.get(url)
resp.text
        # 2. json을 딕셔너리로 형변환하기
jsn = json.loads(resp.content)
jsn["_embedded"]["restaurants"]

        # 3. 필요한 정보 가져와서 데이터프레임으로 저장하기

name, ribbon, year = [], [], []
restaurants = jsn["_embedded"]["restaurants"]

for restaurant in restaurants:
    name.append(restaurant["headerInfo"]["nameKR"])
    ribbon.append(restaurant["headerInfo"]["ribbonType"])
    year.append(restaurant["headerInfo"]["year"])

name,ribbon,year

        # 4. 그럼 위의 코드를 바탕으로 모든 레스토랑의 아래 항목을 모두 파싱하여 csv로 저장해보세요.
tels, dayoffs, parks, prices, menus, times = [], [], [], [], [], []
restaurants = jsn["_embedded"]["restaurants"]

for restaurant in restaurants:
    menus.append(restaurant["statusInfo"]["menu"])
    prices.append(restaurant["statusInfo"]["priceRange"])
    parks.append(restaurant["statusInfo"]["parking"])
    dayoffs.append(restaurant["defaultInfo"]["dayOff"])
    tels.append(restaurant["defaultInfo"]["phone"])
    times.append(restaurant["statusInfo"]["businessHours"])

tels, dayoffs, parks, prices, menus, times

## 03.20 수업-----------------------------------------------------------------------------------------------------------------------

    # 데이터 분석

    # 데이터를 보는 통찰력을 기르자!!
    # 하나의 문제를 끝까지 생각할 수 있는 힘

    # 머신러닝은 cost가 낮은 hypothesis를 찾는 것이다!!!    

    # 머신런닝은?
    #-> cost가 가장 낮을 때의 weight를 찾는 과정!!

import requests
import json
from tqdm.notebook import tqdm
from bs4 import BeautifulSoup

    # 실습 : 한 웹툰 전체 화 전체 댓글 가져오기
box = []
for i in tqdm(range(1,51)):
    url = 'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?'


    params = {'ticket': 'comic',
              'templateId': 'webtoon',
              'pool' : 'cbox3',
              '_cv' : '20240319154009',
              '_callback' : '',
              'lang': 'ko',
              'country' : 'KR',
              'objectId' : f'808198_{i}',
              'pageSize' : '15',
              'indexSize' : '10',
              'groupId' : '808198',
              'listType' : 'OBJECT',
              'pageType' : 'more',
              'page' : '1',
              'currentPage' : '1',
              'refresh' : 'true',
              'sort' : 'new',
              '_': 1710914339145}

    headers = {"referer" : f"https://comic.naver.com/webtoon/detail?titleId=808198&no=50&week=thu",
           "user-agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}
    respond = requests.get(url,params=params, headers=headers)


    # print(len('jQuery3630057019423799983926_1710895256081('))
    # print(respond.text)

    jsn2 = respond.text.replace("_callback(","")[:-2]


    dic2 = json.loads(jsn2)

    total_pages = dic2['result']['pageModel']['totalPages']

    # print(total_pages)

    for j in range(1,total_pages+1):
        url = f'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=comic&templateId=webtoon&pool=cbox3&_cv=20240319154009&_callback=jQuery3630057019423799983926_1710895256081&lang=ko&country=KR&objectId=808198_{i}&categoryId=&pageSize=15&indexSize=10&groupId=808198&listType=OBJECT&pageType=more&page={j}&currentPage={j}&refresh=true&sort=new&_=1710895256093'
        headers = {"referer" : "https://comic.naver.com/webtoon/detail?titleId=808198&no=50&week=thu",
            "user-agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}
        resp = requests.get(url, headers=headers)
        jsn = resp.text.replace("jQuery3630057019423799983926_1710895256081(","").replace(");","")
        dic = json.loads(jsn)

        comments = dic['result']['commentList']

        for comment in comments:
            box.append(comment["contents"])

## 03.21 수업-----------------------------------------------------------------------------------------------------------------------
            
    ## 텍스트 마이닝 -> NLP
    
    ## 패키지 : NLTK / SpaCy
            
    ## 토큰(token)이란 문법적으로 더 이상 나눌 수 없는 언어요소를 뜻함. 텍스트 토큰화(Text Toeknization)란 말뭉치로부터 토큰을 분리하는 작업을 뜻함.
            
    ## 토큰화(Tokenization) : 주어진 텍스트를 원하는 단위(토큰, token)로 나누는 작업을 말함. 
    ## 원하는 단위가 문장인 경우에는 문장 토큰화(Sentence tokenization)라고 하고, 
    ## 단어인 경우에는 단어 토큰화(word tokenization)라고 한다.
            

    # 1) -----spaCy--------------------------------------------
    ## #spaCy는 토큰화에 필요한 데이터를 다운로드 해야함.
    ## python -m spacy download en_core_web_sm
            
import spacy
spacy_en = spacy.load("en_core_web_sm")
text = "David's book wasn't famous, but his family loved his book."
for token in spacy_en(text):
    print(token)

        # spaCy를 이용해 토큰화를 수행하면 기본적으로 토큰외에도 PoS(품사), lemma등의 정보를 알 수 있다.
for token in spacy_en.tokenizer(text):
    print(f"token: {token.text}, PoS: {token.pos_}, lemman: {token.lemma_}")

        # 불용어
stop_words = spacy.lang.en.stop_words.STOP_WORDS   # 자체 불용어 딕셔너리
for i, stop_word in enumerate(stop_words):
    if i == 10:
        break
    print(stop_word)

    # 2) -----NLTK----------------------------------------------
    # NLTK의 Tokenizer(토크나이저)를 사용하기 위해서는 데이터(NLTK Data)를 설치해야한다.

import nltk
nltk.download("popular")

        # 2-1) sentence_tokenizer (점 + 공백 두개를 합친것을 기준으로)
from nltk.tokenize import sent_tokenize
sent_tokenize("I like you. You're my son.")

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))

        # 2-2) word_tokenizer
        # 다만, 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 함.
        # 단어 토큰화는 기본적으로 띄어쓰기를 기준으로 함.
        # 영어보는 보통 띄어쓰기로 토큰이 구분되는 반면, 한국어는 띄어쓰기 만으로 토큰을 구분하기 어려움.

from nltk.tokenize import word_tokenize
nltk.download('punkt')
word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.")
word_tokenize("David's book wasn't famous, but his family loved his book.")

text = input("Please write a text:")
print("The length of the text is",len(word_tokenize(text)),"words")

        # (') 어퍼스트로피가 있는 경우 어떻게 구분할까? -> WordPunctTokenizer = word_tokenize와는 달리 '(어퍼스트로피)를 별도의 토큰으로 구분해서 토큰화를 진행한다.
from nltk.tokenize import WordPunctTokenizer
text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
words = WordPunctTokenizer().tokenize(text)
words

        # (') 어퍼스트로피가 있는 경우 어떻게 구분할까? -> text_to_word_sequence = 모든 알파벳을 소문자로 바꾸고, 구두점(컴마, 마침표 등)을 없애고, 어퍼스트로피(')도 보존하여 토큰을 제대로 구분해줍니다.
from tensorflow.keras.preprocessing.text import text_to_word_sequence
text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
words = text_to_word_sequence(text)
words

    # 3) -----Penn Treebank Tokenization---------------------------------------------
    # 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.
    # 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.

from nltk.tokenize import TreebankWordTokenizer
text = "Starting a home-based restaurant may be an ideal. It doesn't have a food chain or restaurant of their own."
words = TreebankWordTokenizer().tokenize(text)
words




