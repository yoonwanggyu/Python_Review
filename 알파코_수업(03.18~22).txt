## 03.18 수업------------------------------------------------------------------------------------------------------------------------------------------
    # 웹 크롤링 복습
    # 원하는 태그들만의 공통점 찾기 -> 속성 + 계층구조(상위태그 + 특성)

    # 크롤링시, 인덱싱, 슬라이싱을 select와 함께 적극적으로 활용

    # 섭네일 사진 가져오기
    # 책 정보 html 가져오기 -> 해당 정보 태그 가져오기 -> 정보에 있는 url requests로 다시 불러오기 -> 불러운 url 저장

    # 1) 정보와 똑같이 접속
url = requests.get("https://www.aladin.co.kr/shop/common/wbest.aspx?BestType=Bestseller&BranchType=1&CID=170")
soup = BeautifulSoup(url.content,"lxml")

    # 2) 이미지 태그 찾기
img_tag = soup.select("img.front_cover")[0]
print(img_tag)

    # 3) 이미지 태그에서 'src' 만 가져오기
img_src = img_tag['src']

    # 4) 'src'에 있는 url으로 접속해서 requests.get 적용
img_resp = requests.get(img_src)
img_resp.content

    # 5) 파일에 저장
with open('불변의 법칙.jpg','wb') as f:
    f.write(img_resp.content)


    ## iframe은 파싱이 안된다
    ## 해당 주소 들어가서 파싱해야 된다

    # 1) 
url = 'https://finance.naver.com/sise/sise_index_day.naver?code=KOSPI'
resp = requests.get(url)
    # 2)
soup = BeautifulSoup(resp.content,'lxml')
    # 3)
date = [date.text for date in soup.select("td.date")]
price = [price.text for price in soup.select("td.number_1")[::2]][::2]
diff = [('-' + diff.text.strip() if 'red02' not in diff['class'] else diff.text.strip()) for diff in soup.select("td.rate_down span")]
rate = [rate.text.strip() for rate in soup.select("td.number_1 span.tah")]
volumn = [volumn.text.strip() for volumn in soup.select("td.number_1")[2::4]]
amount = [amount.text.strip() for amount in soup.select("td.number_1")[3::4]]
    #4)
df_ksp200 = pd.DataFrame({"날짜":date,
                          "체결가" : price,
                          "전일비": diff,
                          "등락률" : rate,
                          "거래량" : volumn,
                          "거래대금" : amount})
df_ksp200


## 03.19 수업-----------------------------------------------------------------------------------------------------------------------

    # 요청이 거절당했을 때 -> headers를 바꿔서 접근해라
    # 언론사별로 형식이 다르므로 '네이버뉴스' 통합 페이지 href만 보아서 다시 접근

    # javascript 1) disable js 하고 -> 강아지 사진 가져오기
url = requests.get("https://www.google.com/search?q=%EA%B0%95%EC%95%84%EC%A7%80&sca_esv=074560fb4084e5a7&biw=1536&bih=826&gbv=1&tbm=isch&ei=xSz5ZaztCIyk2roPv_yU-Ak&start=0&sa=N")
soup = BeautifulSoup(url.content,"lxml")

img_tags = soup.select("img.DS1iW")
len(img_tags)

for idx,img_tag in enumerate(img_tags):
    img = requests.get(img_tag['src'])

    with open(f"img_{idx}.jpg",'wb') as f:
        f.write(img.content)


    # 크롤링 할 때의 루틴 !!
        
        # 1) 설정 들어가서 disable JS 하기
            # 2) 자바로 바뀐 형태로 페이지가 뜨면 기본 루틴으로
            # 3) 페이지가 뜨지 않는다 -> url 찾기 -> network -> fetch/js -> preview -> General request url 복사
        # 2) iframe 확인
            # 3) 맞다면 태그로 가서 src 에서 기본 루틴으로
        # 3) 앞 상황이 아니라면 기본 루틴 -> 요청 거절이면 headers 바꾸기 


    # javascript 2)
        # 1. 자바 스크립트 페이지 url 찾아서 페이지 가져오기
import requests
import json
from tqdm.notebook import tqdm

url = "https://www.bluer.co.kr/api/v1/restaurants?page=0&size=30&query=&foodType=&foodTypeDetail=&feature=&location=&locationDetail=&area=&areaDetail=&priceRange=&ribbonType=&recommended=false&isSearchName=false&tabMode=single&searchMode=ribbonType&zone1=&zone2=&zone2Lat=&zone2Lng="
resp = requests.get(url)
resp.text
        # 2. json을 딕셔너리로 형변환하기
jsn = json.loads(resp.content)
jsn["_embedded"]["restaurants"]

        # 3. 필요한 정보 가져와서 데이터프레임으로 저장하기

name, ribbon, year = [], [], []
restaurants = jsn["_embedded"]["restaurants"]

for restaurant in restaurants:
    name.append(restaurant["headerInfo"]["nameKR"])
    ribbon.append(restaurant["headerInfo"]["ribbonType"])
    year.append(restaurant["headerInfo"]["year"])

name,ribbon,year

        # 4. 그럼 위의 코드를 바탕으로 모든 레스토랑의 아래 항목을 모두 파싱하여 csv로 저장해보세요.
tels, dayoffs, parks, prices, menus, times = [], [], [], [], [], []
restaurants = jsn["_embedded"]["restaurants"]

for restaurant in restaurants:
    menus.append(restaurant["statusInfo"]["menu"])
    prices.append(restaurant["statusInfo"]["priceRange"])
    parks.append(restaurant["statusInfo"]["parking"])
    dayoffs.append(restaurant["defaultInfo"]["dayOff"])
    tels.append(restaurant["defaultInfo"]["phone"])
    times.append(restaurant["statusInfo"]["businessHours"])

tels, dayoffs, parks, prices, menus, times

## 03.20 수업-----------------------------------------------------------------------------------------------------------------------

    # 데이터 분석

    # 데이터를 보는 통찰력을 기르자!!
    # 하나의 문제를 끝까지 생각할 수 있는 힘

    # 머신러닝은 cost가 낮은 hypothesis를 찾는 것이다!!!    

    # 머신런닝은?
    #-> cost가 가장 낮을 때의 weight를 찾는 과정!!

import requests
import json
from tqdm.notebook import tqdm
from bs4 import BeautifulSoup

    # 실습 : 한 웹툰 전체 화 전체 댓글 가져오기
box = []
for i in tqdm(range(1,51)):
    url = 'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?'


    params = {'ticket': 'comic',
              'templateId': 'webtoon',
              'pool' : 'cbox3',
              '_cv' : '20240319154009',
              '_callback' : '',
              'lang': 'ko',
              'country' : 'KR',
              'objectId' : f'808198_{i}',
              'pageSize' : '15',
              'indexSize' : '10',
              'groupId' : '808198',
              'listType' : 'OBJECT',
              'pageType' : 'more',
              'page' : '1',
              'currentPage' : '1',
              'refresh' : 'true',
              'sort' : 'new',
              '_': 1710914339145}

    headers = {"referer" : f"https://comic.naver.com/webtoon/detail?titleId=808198&no=50&week=thu",
           "user-agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}
    respond = requests.get(url,params=params, headers=headers)


    # print(len('jQuery3630057019423799983926_1710895256081('))
    # print(respond.text)

    jsn2 = respond.text.replace("_callback(","")[:-2]


    dic2 = json.loads(jsn2)

    total_pages = dic2['result']['pageModel']['totalPages']

    # print(total_pages)

    for j in range(1,total_pages+1):
        url = f'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=comic&templateId=webtoon&pool=cbox3&_cv=20240319154009&_callback=jQuery3630057019423799983926_1710895256081&lang=ko&country=KR&objectId=808198_{i}&categoryId=&pageSize=15&indexSize=10&groupId=808198&listType=OBJECT&pageType=more&page={j}&currentPage={j}&refresh=true&sort=new&_=1710895256093'
        headers = {"referer" : "https://comic.naver.com/webtoon/detail?titleId=808198&no=50&week=thu",
            "user-agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}
        resp = requests.get(url, headers=headers)
        jsn = resp.text.replace("jQuery3630057019423799983926_1710895256081(","").replace(");","")
        dic = json.loads(jsn)

        comments = dic['result']['commentList']

        for comment in comments:
            box.append(comment["contents"])





