# 03.25------------------------------------------------------------------------------------------------------------####

    # 정수 인코딩-----------------------------------------------------------------------##
    # 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고 => 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다.
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

        # 1) 문장 토큰화
nltk.download('popular')
text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."
text = sent_tokenize(text)
text
        # 2) 각 문장을 단어 토큰화 후 불용어 제외한 단어들만 추출
vocab = {}
sentences = []
stopwords = set(stopwords.words('english'))

for i in text:
    sentence = word_tokenize(i)
    result = []

    for word in sentence:
        word = word.lower()
        if word not in stopwords:
            if len(word) > 2:
                result.append(word)
                if word not in vocab:
                    vocab[word] = 0
                vocab[word] += 1
    sentences.append(result)

sentences
vocab.items()

        # 3) 빈도수에 따른 정렬
vocab_sorted = sorted(vocab.items(), key = lambda x : x[1], reverse=True)
vocab_sorted

        # 4) 빈도가 1인 단어 제외 -> 다시 빈도수가 많을 수록 1 부여
word_to_index = {}
i = 0
for (word,freq) in vocab_sorted:
    if freq > 1:
        i += 1
        word_to_index[word] = i

word_to_index

        # 5) 상위 5개만 추출
word_freq = [w for w,c in word_to_index.items() if c >= 6]
for w in word_freq:
    del word_to_index[w]
word_to_index

        # 6) 이제 word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장되었습니다. 
        #    이제 word_to_index를 사용하여 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업을 하겠습니다.
        #    Out-Of-Vocabulary(단어 집합에 없는 단어)의 약자로 'OOV'라고 합니다.
word_to_index['OOV'] = len(word_to_index) + 1       # 단어 집합에 없는 단어는 6으로 정수 인코딩
encoded = []
for s in sentences:
    temp = []
    for w in s:
        try:
            temp.append(word_to_index[w])
        except KeyError:
            temp.append(word_to_index['OOV'])
    encoded.append(temp)

encoded


    # counter()함수 사용---------------------------------##
from collections import Counter
sentences
        # 1) 하나의 리스트에 단어들 나열
words = sum(sentences,[])
        # 2) 함수로 한번에 갯수 카운트 가능
vocab = Counter(words)
        # 3) 상위 5개만 출력
vocab = vocab.most_common(5)
        # 4) 정수 인코딩 / 빈도수가 높은 순서대로 1~
vocab_to_index = {}
i = 0
for w,f in vocab:
    i += 1
    vocab_to_index[w] = i

vocab_to_index


    # (*args) / (*kwargs) 이용 => 여러개의 인자를 받기 위해--------------------------------------------------------------##
        # 리스트는 계산이 안됨 => 각각 계산이 안되기 때문에 오류 => 적용 가능한 함수를 만들어 줘야 함!!

def test(*args): #args : argument *(Asterisk) : 개수가 정해지지 않은 변수를 함수의 파라미터로 사용하는 것.
    #args:tuple
    print(args)
test(1,2,3,4)

def add_all(*inputs):
    return sum(inputs)
add_all(1,2,3,4,5,6,7,8,9,10)

def add_all(*inputs):
    add = 0
    for i in inputs:    # 행에 접근
        for j in i:     # 열에 접근
            add += j
    print(add)
add_all([1,2,3,4,5,6,7,8,9,10])

def add_all(*inputs):   #  list와 tuple을 동시에 받을 수 있는 코드
    add = 0
    for i in range(len(inputs)):
        if type(inputs[i]) == list:
            for j in inputs[i]:
                add += j
        else:
            add += inputs[i]
    return add
print(add_all(1,2,3,4,5,6,7,8,9,10))
print(add_all([1,2,3,4,5,6,7,8,9,10]))

def dict_1(**kwargs):  # 매개변수가 몇개 입력으로 오는지 모를 때 -> 딕셔너리로 반환
    print(kwargs)
dict_1(a=1,b=2,c=3)


    # NLTK의 FreqDist---------------------------------------------------------------------------------##
    # FreqDist의 입력으로는 반드시 토큰화가 이루어진 상태여야 한다.

from nltk import FreqDist
import numpy as np
sentences = ['barber', 'barber', 'person', 'barber', 'good', 'person']

        # np.hstack으로 문장 구분을 제거하여 입력으로 사용 . ex) ['barber', 'person', 'barber', 'good' ... 중략 ...
vocab = FreqDist(np.hstack(sentences))   # np.hstack() 함수는 주어진 배열들을 수평(가로)으로 합치는 기능을 해요! 이 함수는 1차원 배열을 수평(가로)으로 쌓을 때 많이 사용

vocab = vocab.most_common(5)

        # 정수 인코딩
word_to_index = {word[0] : index + 1 for index,word in enumerate(vocab)}


    # 품사 태깅-------------------------------------------------------------------------------------------------##
    # 문장을 형태소 단위로 분리 한 후, 해당 형태소의 품사를 태깅하는 것을 의미

from nltk.tag import pos_tag
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
word = word_tokenize(text)
pos_tag(word)


# 03.26---------------------------------------------------------------------------------------------------------------------------------------####

    # kolaw : 헌법 말뭉치
from konlpy.corpus import kolaw
law = kolaw.open('constitution.txt').read()
law[:40]

    # kobill : 국회 법안 말뭉치
from konlpy.corpus import kobill
bill = kobill.open('1809890.txt').read()
bill[:40]

from konlpy.tag import Okt
from konlpy.tag import Komoran
from konlpy.tag import Kkma
from konlpy.tag import Hannanum

hannaum = Hannanum()
kkma = Kkma()
okt = Okt()
komoran = Komoran()
        # 명사 추출
print(hannaum.nouns(law[:40]))
print(kkma.nouns(law[:40]))
print(okt.nouns(law[:40]))
print(komoran.nouns(law[:40]))
        # 품사 부착
print(hannaum.pos(law[:40]))
print(kkma.pos(law[:40]))
print(okt.pos(law[:40]))
print(komoran.pos(law[:40]))

    # SONLPY-------------------------------------------------------------------------------------------------------------##
    # 텍스트 데이터에서 특정 문자 시퀀스가 함께 자주 등장하는 빈도가 높고, 
    # 앞 뒤로 조사 또는 완전히 다른 단어가 등장하는 것을 고려해서 해당 문자 시퀀스를 형태소라고 판단하는 단어 토크나이저라면 어떨까요?
    # soynlp는 기본적으로 학습에 기반한 토크나이저이므로 학습에 필요한 한국어 문서를 다운로드합니다.

import urllib.request
from soynlp import DoublespaceLineCorpus
from soynlp.word import WordExtractor
urllib.request.urlretrieve("https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt", filename="2016-10-20.txt")

    # 훈련 데이터를 다수의 문서로 분리
corpus = DoublespaceLineCorpus("2016-10-20.txt")
len(corpus)  # 문서의 갯수

    # 다수의 문서에서 3개만 가져오기
i = 0
for document in corpus:
    if len(document) > 0:
        print(document)
        i += 1
    if i == 3:
        break

    # 단어 추출
word_extractor = WordExtractor()
word_extractor.train(corpus)                  # 형태소에 해당하는 단어를 분리하는 학습을 수행
word_score_table = word_extractor.extract()

    # SOYNLP의 응집 확률
    # cohesion : cohesion값이 가장 큰 위치가 하나의 단어를 이루고 있을 가능성이 높다.
    # 조건부 확률 기반
word_score_table['한'].cohesion_forward
word_score_table['한강'].cohesion_forward
word_score_table['한강공'].cohesion_forward
word_score_table['한강공원'].cohesion_forward
word_score_table['반포한강공원'].cohesion_forward


    # 언어모델 : 단어 시퀀스에 확률을 할당하는 모델
    # N-GRAM : 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법
from nltk import ConditionalFreqDist
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import numpy as np
import nltk
nltk.download('punkt')
nltk.download('movie_reviews')
from nltk.corpus import movie_reviews

        # bigram 모델
sentence = "I am a boy"
word = word_tokenize(sentence)
bigram = ngrams(word,2,pad_left=True,pad_right=True,left_pad_symbol="<s>",right_pad_symbol="</s>")
list(bigram)
cfd = ConditionalFreqDist([(t[0],t[1]) for t in bigram])

sentence = []
for token in movie_reviews.sents():
    bigrams = ngrams(token,2,pad_left=True,pad_right=True,left_pad_symbol="<s>",right_pad_symbol="</s>")
    sentence += [t for t in bigrams]

cfd = ConditionalFreqDist([(t[0],t[1]) for t in bigrams])
cfd = ConditionalFreqDist(sentence)
cfd["<s>"].most_common(5)
sentence[:20]


    # 단어의 표현 방법
    # BoW : 국소 표현(해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법)에 속하며, 단어의 빈도수를 카운트(Count)하여 단어를 수치화하는 단어 표현 방법
    # 단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법

        # bow 함수
def build_bag_of_words(document):
    document = document.replace(",","")
    tokenized_document = okt.morphs(document)

    word_to_index ={}
    bow = []

    for word in tokenized_document:
        if word not in word_to_index.keys():
            word_to_index[word] = len(word_to_index)
            # BoW에 전부 기본값 1을 넣는다.
            bow.insert(len(word_to_index)-1,1)
        else:
            # 재등장하는 단어의 인덱스
            index = word_to_index.get(word)
            bow[index] = bow[index] + 1
    
    return word_to_index,bow

doc1 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
vocab , bow = build_bag_of_words(doc1)
print('vocabulary :', vocab)
print('bag of words vector :', bow)

        # 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰입니다. 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰입니다.


    # CountVectorizer 클래스로 BoW 만들기
    # 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['you know I want your love. beacuse I love you']
vector = CountVectorizer()
# 코퍼스로부터 각 단어의 빈도수를 기록
print("bag of words vector:",vector.fit_transform(corpus).toarray())
# 각 단어의 인덱스가 어떻게 부여되었는지를 출력
print("vocabulary:",vector.vocabulary_)


    # 불용어를 제거한 BoW 만들기
from nltk.corpus import stopwords
nltk.download("stopwords")

text = ["Family is not an important thing. It's everything"]
# 1) 사용자가 직접 정의한 불용어 사용
vect = CountVectorizer(stop_words=['the','a','an','is','not'])
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
# 2) CountVectorizer에서 제공하는 자체 불용어 사용
vect = CountVectorizer(stop_words='english')
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
# 3) NLTK에서 지원하는 불용어 사용
sw = stopwords.words("english")
vect = CountVectorizer(stop_words=sw)
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
