# 4/12------------------------------------------------------------------------------------------------------------------------------------------

        # 하이퍼_파라미터_최적화

    # 1) GridSearchCV-------------------------------------------

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from xgboost import XGBClassifier

iris = load_iris()
X = iris.data
y = iris.target
clf = XGBClassifier()

#Importing packages from sklearn
from sklearn import preprocessing
from sklearn import model_selection
from sklearn import metrics

#defining a set of values as a dictionary for hyperparameters
param_grid = {
    "n_estimators":[100,200,300,400],
    "max_depth":[1,3,5,7],
    "reg_lambda":[.01,.1,.5]    
}

#declaring GridSearchCV model
model = model_selection.GridSearchCV(
    estimator = clf,
    param_grid = param_grid,
    scoring = 'accuracy',
    verbose = 10,
    n_jobs = 1,
    cv = 5    
)

#fitting values to the gridsearchcv model
model.fit(X,y)

#printing the best possible values to enhance accuracy
print(model.best_params_)
print(model.best_estimator_)

#printing the best score
print(model.best_score_)

# model save
    # import joblib
    # joblib.dump(model,'model_xgboost.pkl')
    # joblib.load('model_xgboost.pkl')


    # 2) RandomizedSearchCV-------------------------------

#defining a set of values as a dictionary for hyperparameters
param_grid = {
    "n_estimators":[100,200,300,400],
    "max_depth":[1,3,5,7],
    "reg_lambda":[.01,.1,.5]    
}

#declaring RandomizedSearchCV model
model = model_selection.RandomizedSearchCV(
    estimator = clf,
    param_distributions = param_grid,
    scoring = 'accuracy',
    verbose = 10,
    n_jobs = 1,
    cv = 5,
    n_iter=10
)

#fitting values to the RandomizedSearchCV model
model.fit(X,y)

#printing the best possible values to enhance accuracy
print(model.best_params_)
print(model.best_estimator_)

#printing the best score
print(model.best_score_)


    # 3) Bayesian optimization-----------------------------


    # 4) Hyperopt--------------------------------------------


    # 5) Optuna-------------------------------------------
        # 실제로 가장 많이 씀
import optuna
from functools import partial

def optimize(trial,x,y):
    reg_lambda = trial.suggest_float('reg_lambda',0.01,1)
    n_estimators = trial.suggest_int("n_estimators",100,150)
    max_depth = trial.suggest_int("max_depth",3,15)
    max_features = trial.suggest_float("max_features",0.01,1)

    model = XGBClassifier(n_estimators = n_estimators,
                          reg_lambda = reg_lambda,
                          max_depth = max_depth,
                          max_features = max_features)
    
    kf = model_selection.StratifiedKFold(n_splits=5)

    accuracies = []

    for i in kf.split(X=x,y=y):
        train_i,test_i = i[0],i[1]
        
        xtrain = x[train_i]
        ytrain = y[train_i]

        xtest = x[test_i]
        ytest = y[test_i]

        model.fit(xtrain,ytrain)

        predicts = model.predict(xtest)

        fold_accuracy = metrics.accuracy_score(ytest,predicts)

        accuracies.append(fold_accuracy)

    return np.mean(accuracies)

optimization_function = partial(optimize,x=X,y=y)
study = optuna.create_study(direction = 'maximize')

study.optimize(optimization_function,n_trials=10)

print(f"Best trial Score : {study.best_trial.value} \n Params : {study.best_trial.params}")

import matplotlib.pyplot as plt
from optuna.visualization import plot_edf
from optuna.visualization import plot_optimization_history
from optuna.visualization import plot_parallel_coordinate
from optuna.visualization import plot_param_importances
from optuna.visualization import plot_slice

optuna.visualization.plot_param_importances(study)
optuna.visualization.plot_optimization_history(study)


    # (특강) 4.12 ------------------------------------------------------------------------------

        # LLM의 핵심 구조 - 트랜스포머(Transformer)

        # 트랜스포머 디코더를 사용해 GPT3(빈칸 단어 예측)를 만듦
        # -> GPT3를 이용해 챗봇을 만든게 GTP3.5(강화학습으로)

        # langchain으로 LLM 서비스 개발하기

# LangChain 활용 - 
    # RAG 사용 : 문서 검색기와 LLM을 통합한 것

    # 실습(만드는 순서)

        # 1) 문서 불러오기
from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("C:/Users/Administrator/Downloads/소나기 - 황순원.pdf")
document = loader.load()

        # 2) 문서 분할
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
texts = text_splitter.split_documents(document)
texts[1].page_content

        # 3) 문서 내용 임베딩한 뒤 Chroma DB 벡터스토어에 저장
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

embeddings_model = OpenAIEmbeddings()
docsearch = Chroma.from_documents(texts,embeddings_model)

        # 4) retriever 가져오기
retriever = docsearch.as_retriever()

        # 5) rag 프롬프트 다운로드
from langchain import hub

rag_prompt = hub.pull("rlm/rag-prompt")

        # 6) ChatGPT 모델 지정
from langchain_openai import ChatOpenAI

OPEN_API_KEY = " "
lim_ = ChatOpenAI(temperature= 0,
                 model_name = 'gpt-3.5-turbo',
                 openai_api_key = OPEN_API_KEY)

        # 7) pipe operator를 활용한 체인 생성
from langchain.schema.runnable import RunnablePassthrough

rag_chain = ({"context" : retriever, "question": RunnablePassthrough()
              | rag_prompt
              | lim_})

        # 8) 질문해보기
rag_chain.invoke("소녀는 어떤 옷을 입고 있어?")


    # 임의로 해보기----------------------------

# 프롬프트 생성
# PromptTemplate은 LLM에 문장을 전달하기 전에 문장 구성을 편리하게 만들어주는 역할을 합니다.
from langchain import PromptTemplate
template = "{product}를 홍보하기 위한 좋은 문구를 추천해줘"

prompt = PromptTemplate(input_variables= ['product'],
                        template = template)

prompt.format(product="노트북")

            
            # 코사인 유사도 함수 정의
from numpy import dot
from numpy.linalg import norm
import numpy as np

def cos_sim(A, B):
       return dot(A, B)/(norm(A)*norm(B))

            # 문서 임베딩 -> 코사인 유사도 측정 -> DB에 저장 -> retriever 사용해 질문
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

sentences = [
    "안녕하세요",
    "제 이름은 홍길동입니다.",
    "이름이 무엇인가요?",
    "랭체인은 유용합니다.",
    "홍길동 아버지의 이름은 홍상직입니다."
    ]

embeddings_model = OpenAIEmbeddings()
db = Chroma.from_texts(
    sentences,
    embeddings_model,
    collection_name = 'history',
    persist_directory = './db/chromadb',
    collection_metadata = {'hnsw:space': 'cosine'}, # l2 is the default
)

# 가장 유사도가 높은 문장을 하나만 추출
retriever = db.as_retriever(search_kwargs={'k': 1})
query = '홍길동 아버지의 이름은 무엇입니까?'
retriever.get_relevant_documents(query)
