# 04.15-----------------------------------------------------------------------------------------------------------------------------------------------

    # 1) AND 연산 퍼셉트론 구현
        # 둘다 1 일땐 -> 1
        # 나머지는 다 -> 0
import numpy as np

def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5]) # 초기값
    b = -0.7                # 절편
    tmp = np.sum(w*x) + b   # y값 = sum((w^T * X) + b)

    if tmp <= 0:
        return 0
    else:
        return 1
    
data = []
for xs in [(0,0),(1,0),(0,1),(1,1)]:
    data.append(xs)
    print(data[0])
    y = AND(xs[0],xs[1])
    print(str(xs) + "->" + str(y))


    # 2) NAND 연산 퍼셉트론 구현
        # AND연산자 반대 
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp =  np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = NAND(xs[0], xs[1])
    print(str(xs) + " -> " + str(y))


    # 3) OR 연산자 퍼셉트론 구현
        # 둘중에 하나라도 1이면 -> 1
        # (0,0) -> 0
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([2,2])
    b = -1
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
    
for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = OR(xs[0],xs[1])
    print(str(xs) + "->" + str(y))


    # 4) XOR 연산 다층 퍼셉트론 구현
        # (0,0) -> 0
        # (0,1) -> 1
        # (1,0) -> 1
        # (1,1) -> 0
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y

print("NAND, OR, AND")
for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = XOR(xs[0], xs[1])
    print(str(xs) + " -> " + str(y))


    # 5) 텐서플로우 익히기
        # 자동 미분
import tensorflow as tf
x = tf.Variable(0.)  # 변수 x에 초기값으로 0.0을 할당하는 것
    # TensorFlow에서 변수를 사용할 때는 그 변수를 사용하기 전에 초기화해야 합니다. 이는 TensorFlow가 변수에 대한 메모리를 할당하고 해당 변수를 사용할 준비를 하기 위함입니다. 
    # TensorFlow의 tf.Variable() 함수를 사용하여 변수를 선언하고 초기화할 수 있습니다. 그렇기 때문에 코드에서 x를 초기화하는 것은 그저 TensorFlow에게 이 변수를 사용할 것이라고 알려주는 것과 같습니다.

with tf.GradientTape() as tape: 
    y = 2 * x +3
grad_of_y_wrt_x = tape.gradient(y,x)  # y를 x에 대해서 미분
print(grad_of_y_wrt_x)


    # 6) Sequential API : 순차적 모델
tf.random.set_seed(777)
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import mse

    # 데이터 준비하기
        # 1-1) XOR -> 활성화 함수 : linear -> 직선 하나로는 XOR를 맞출 수 없음
x = np.array([[0,0],[1,0],[0,1],[1,1]])
y = np.array([[0],[1],[1],[0]])

model = Sequential()
model.add(Dense(1,input_shape = (2,),activation = 'linear'))    # layers.Dense랑 같은 의미(명시적으로 layers를 쓸 수도 있음 : 가독성)
model.compile(optimizer = 'SGD',    # 확률적 경사하강법 : 손실 함수를 최소화하기 위해 모델의 가중치를 업데이트
              loss = mse,           # mse 손실함수 사용 : 평균 제곱 오차 -> 회귀 문제에서 주로 사용 -> 차이가 작을수록 좋음
              metrics = ['acc'])    # 모델 성능 지표 : 정확도 -> 분류 문제에서 주로 사용
model.fit(x,y,epochs=500)
model.summary()
model.get_weights()

    # 1-2) XOR -> 층 2개 -> 활성화 함수 : sigmoid 
tf.random.set_seed(777) # 시드를 설정 함.

# 데이터 준비하기
x = np.array([[0,0],[1,0],[0,1],[1,1]])
y = np.array([[0],[1],[1],[0]])

# XOR gate이기 때문에 y값은 0,1,1,0임

#모델 구성하기
model = Sequential()
model.add(Dense(32, input_shape = (2, ), activation = 'relu')) # 32개로 여러개로 쌓음.
model.add(Dense(1, activation='sigmoid')) # 이진분류 모델 = sigmoid -> 3개부터는 softmax 사용
                                          # 출력층에 존재하는 뉴런의 수

#모델 준비하기
model.compile(optimizer = 'adam',   # Adam : 각 가중치마다 개별적인 학습률 유지 / 각 가중치의 경사를 추적 / 지역 최솟값이 아닌 전역 최솟값을 탐색 / 각 반복 단계에서 편향을 보정하여 초기 반복에서 학습률을 너무 높게 설정하는 것을 막아줌
              loss = mse,
              metrics = ['acc'])    # list형태로 평가지표를 전달

#학습시키기
model.fit(x,y,epochs = 100)
# 시각화
from tensorflow.keras.utils import plot_model
plot_model(model, show_shapes=True)
plt.show()


    # 7) Activation Functions
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def relu(z):
    return np.maximum(0, z)

def derivative(f, z, eps=0.000001):
    return (f(z + eps) - f(z - eps))/(2 * eps)
import matplotlib.pyplot as plt

z = np.linspace(-5, 5, 200)

plt.figure(figsize=(11,4))

plt.subplot(121)
plt.plot(z, np.sign(z), "r-", linewidth=1, label="Step")
plt.plot(z, sigmoid(z), "g--", linewidth=2, label="Sigmoid")
plt.plot(z, np.tanh(z), "b-", linewidth=2, label="Tanh")
plt.plot(z, relu(z), "m-.", linewidth=2, label="ReLU")
plt.grid(True)
plt.legend(loc="center right", fontsize=14)
plt.title("Activation functions", fontsize=14)
plt.axis([-5, 5, -1.2, 1.2])

#plt.legend(loc="center right", fontsize=14)
plt.title("Derivatives", fontsize=14)
plt.axis([-5, 5, -0.2, 1.2])

plt.show()


    # 8) Functional API -> sequential 모델에 비해 복잡하고 다양한 유형의 연결을 다룰 때 사용 / 다중 입력(출력) 모델 
## data 선언
x_data = np.array([[0.0,0.0], [0.0,1.0], [1.0,0.0],[1.0,1.0]])
y_data = np.array([[0], [1], [1], [0]])

    # 입력층
input_layer = tf.keras.layers.Input(shape=(2,))
    # 은닉층
x = tf.keras.layers.Dense(2,activation='sigmoid')(input_layer)
    # 출력층
Out_layer = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    # 모델 정의
model = tf.keras.Model(inputs=[input_layer],outputs=[Out_layer])
    # 모델 확인
model.summary()

optimizer = tf.keras.optimizers.SGD(learning_rate=0.7)
loss = tf.keras.losses.binary_crossentropy              # 이진분류 문제에 사용 : 실제값과 예측값 사이의 교차 엔트로피 계산
metrics = tf.keras.metrics.binary_accuracy              # 예측한 이진 레이블이 실제 레이블과 일치하는 비율 계산

model.compile(loss = loss, optimizer = optimizer, metrics = [metrics])

model.fit(x_data,y_data,epochs=100, batch_size=4)

    # custom model
input_A = tf.keras.layers.Input(shape=[5],name='wide_input')
input_B = tf.keras.layers.Input(shape=[6],name='deep_input')
hidden1 = tf.keras.layers.Dense(30,activation='relu')(input_B)
hidden2 = tf.keras.layers.Dense(30,activation='relu')(hidden1)
concat = tf.keras.layers.concatenate([input_A,hidden2])
output = tf.keras.layers.Dense(1,name='output')(concat)     # activation기본값은 Linear
model = tf.keras.Model(inputs=[input_A,input_B],outputs=[output])
model.summary()


    # 9) Class 수업
        # class : 객체를 만드는 구조/틀
        # instance : 클래스가 실질적으로 객체를 만들었을 때, 그 객체를 부르는 용어
class CustomNumers:
    def __init__(self):
        self._numbers = [n for n in range(1,11)]

a = CustomNumers()
a[2:5]      # 불가
a._numbers[2:5]

class CustomNumers:
    def __init__(self):
        self._numbers = [n for n in range(1,11)]
    
    def __getitem__(self,idx):
        return self._numbers[idx]
    
a = CustomNumers()
a[2:5]      # 가능 -> __getitem__ 때문

    # 10) MPG 데이터로 실습 -> colab으로 실습
https://colab.research.google.com/drive/1g1azaxxvabXN9PzgNqJdWgGb1DcwF5KM?usp=sharing


# 04.16-----------------------------------------------------------------------------------------------------------##

    # 선형 회귀를 기반 심층 신경망 예제 코드를 작성하고 실습해보자---------
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

x_train = np.array([1,2,3,4,5])
y_train = np.array([2,4,6,8,10])

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(1,)),
    tf.keras.layers.Dense(5,activation='relu')
])

model.add(tf.keras.layers.Dense(3,activation="relu"))
model.add(tf.keras.layers.Dense(1))

model.summary()

optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)


    # MNIST 데이터 셋 -----------
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

X_train.shape
y_train.shape
X_test.shape
y_test.shape

        # 데이터 그림으로 확인
sample_size = 10
random_idx = np.random.randint(60000,size=sample_size)
for idx in random_idx:
    img = X_train[idx, :]
    label = y_train[idx]
    plt.figure()
    plt.imshow(img)
    plt.title(f"{idx}-th data, label is {label}")
    plt.show()

        # X_train에서 검증 데이터 만들기
from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.3,random_state=777)
print(f"훈련 데이터 {X_train.shape} 레이블 {y_train.shape}")
print(f"검증 데이터 {X_val.shape} 레이블 {y_val.shape}")

        # 모델 입력을 위해 데이터를 784차원으로 변경
num_X_train = X_train.shape[0]
num_X_val = X_val.shape[0]
num_X_test = X_test.shape[0]

X_train = (X_train.reshape((num_X_train,28*28))) / 255
X_val = (X_val.reshape((num_X_val,28*28))) / 255
X_test = (X_test.reshape((num_X_test,28*28))) / 255
X_train.shape
y_train     # 답

        # 원핫인코딩 -> 답인것만 1로 표시 -> 모델은 몇번째가 답인지 모름
from tensorflow.keras.utils import to_categorical
num_classes = 10
y_train = to_categorical(y_train,num_classes)
y_val = to_categorical(y_val,num_classes)
y_test = to_categorical(y_test,num_classes)
y_train

        # 모델 구성하기
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense # type: ignore

model = Sequential([
    tf.keras.layers.Input(shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu')
])
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))
model.summary()

        # 학습과정 설정하기
model.compile(optimizer='adam',
              loss="categorical_crossentropy",      # 위에서 답을 원핫인코딩 안했을 경우 sparse_를 붙여서 쓰기(sparse_categorical_crossentropy)
              metrics=['acc'])

        # 모델 학습하기
history = model.fit(X_train,y_train,
                    epochs=30,
                    batch_size=128,
                    validation_data=(X_val,y_val),
                    verbose = 1)

        # 결과 시각화
def plot_history(history):
    plt.figure(figsize=(10,5))

    epochs = range(1,len(history.history['loss'])+1)

    plt.subplot(1,2,1)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.plot(epochs,history.history['loss'],label="train_loss")
    plt.plot(epochs,history.history['val_loss'],label="val_loss")
    plt.legend()
    plt.title("train and val loss")

    plt.subplot(1,2,2)
    plt.xlabel("Epochs")
    plt.ylabel("Acc")
    plt.plot(epochs,history.history['acc'],label="train_acc")
    plt.plot(epochs,history.history['val_acc'],label="val_acc")
    plt.legend()
    plt.title("train and val acc")

    plt.show()

plot_history(history)

        # 모델 평가
model.evaluate(X_test,y_test)

results = model.predict(X_test)
results[0]      # 10개에 클래스에 대한 확률 분포를 출력
results.shape
np.set_printoptions(precision=7)    # 소숫점 7자리
print(f"각 클래스에 속할 확률 : {results[0]}")

arg_results = np.argmax(results,axis=-1)    # 10개의 클래스에 대한 확률 분포중 가장 높은 값 출력 / axis=-1은 배열의 마지막 차원(여기서는 클래스에 해당하는 차원)을 따라 최대값을 찾도록 지정
plt.imshow(X_test[0].reshape(28,28))
plt.title("Predicted value of the first image :" + str(arg_results[0]))
plt.show()

        # 모델 평가 -> 혼동행렬
from sklearn.metrics import classification_report,confusion_matrix
import seaborn as sns

plt.figure(figsize=(7,7))
cm = confusion_matrix(np.argmax(y_test,axis=-1),np.argmax(results,axis=-1))
sns.heatmap(cm,annot=True,fmt='d',cmap='Blues')
plt.xlabel('predicted label')
plt.ylabel('true label')
plt.show()

        # 모델 평가 -> 분류 보고서
print('\n', classification_report(np.argmax(y_test, axis = -1), np.argmax(results, axis = -1)))


    # Fashion MNIST------------------------------------------
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images.shape
test_images.shape
train_labels

img = train_images[0,:]
label = train_labels[0]
plt.imshow(img)
plt.title(f"첫번째 사진: {label}")
plt.show()

train_images = train_images / 255.0
test_images = test_images / 255.0

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])  # 눈금 제거
    plt.yticks([])
    plt.grid(False) # 그리드 제거
    plt.imshow(train_images[i],cmap=plt.cm.binary) # cmap=plt.cm.binary는 흑백 이미지로
    plt.xlabel(class_names[train_labels[i]])
    plt.show()

from tensorflow.keras.layers import Flatten

model = Sequential([
    Flatten(input_shape = (28,28)),
    Dense(64,activation='relu'),
    Dense(32,activation='relu'),
    Dense(10,activation='softmax')
])

model.compile(optimizer='adam',
              loss = "sparse_categorical_crossentropy",
              metrics = ['acc'])

model.fit(train_images,train_labels,epochs=10,validation_split=0.2)

predictions = model.predict(test_images)
predictions[0]
np.argmax(predictions[0])
test_labels[0]



    # IMDB 이진 분류----------------------------------------------------
from keras.datasets import imdb
from keras.callbacks import ReduceLROnPlateau,EarlyStopping
from keras.layers import BatchNormalization,Dropout
from keras.regularizers import l2

(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=10000)
max([max(sequence) for sequence in train_data])

        # 원핫인코딩
def vectorize_sequences(sequences,dimension=10000):
    result = np.zeros((len(sequences),dimension))     # 모든 요소가 0인 배열 생성(밑그림) / len(sequences)는 배열의 행의 개수 / dimension은 배열의 열의 개수
    for i,sequences in enumerate(sequences):
        result[i,sequences] = 1.0
    return result

        # 데이터 원핫인코딩
X_train = vectorize_sequences(train_data)
X_test = vectorize_sequences(test_data)
y_train = np.asarray(train_labels).astype(float)
y_test = np.asarray(test_labels).astype(float)

        # 모델 생성
imdb = Sequential()
imdb.add(Dense(32,input_shape=(10000,),activation='relu', kernel_initializer='he_normal',kernel_regularizer=l2(0.001)))
# imdb.add(BatchNormalization())
# imdb.add(Dropout(0.2))
# imdb.add(Dense(32,activation='relu',kernel_regularizer=l2(0.001)))
imdb.add(Dense(1,activation="sigmoid"))

imdb.compile(loss = "binary_crossentropy",
             optimizer='adam',
             metrics=['acc'])
        # 모델 훈련
X_valid = X_train[:10000]
partial_X_train = X_train[10000:]
y_valid = y_train[:10000]
partial_y_train = y_train[10000:]

            # 학습률을 감소시키는 콜백
                # monitor : 검사할 지표
                # factor: 학습률을 감소시킬 비율을 지정 / 기본값은 0.1
                # patience: 지정된 지표(monitor)가 개선되지 않은 에포크의 수를 지정 / 이 수만큼 지표가 개선되지 않으면 학습률이 감소됩니다.
                # min_lr: 학습률의 하한입니다. 학습률은 이 값보다 낮아지지 않습니다.
reduce_Lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,
                              min_lr=0.0001)

            # 모니터링된 지표가 개선되지 않을 때 트레이닝을 중지
                # patience: 검증 손실이 개선되지 않는 에포크 수 /  예를 들어, 10으로 설정하면 검증 손실이 10회 연속 개선되지 않으면 트레이닝이 중지됩니다.
                # verbose: 상세 모드입니다. 1로 설정하면 조기 중지로 트레이닝이 중지될 때 메시지를 출력합니다.
                # restore_best_weights: 모델의 가중치를 모니터링된 지표의 최상의 값을 가진 에포크로 복원할지 여부입니다. True로 설정하면 모델 가중치가 검증 세트에서 최상의 성능을 보인 에포크의 값으로 설정됩니다.
early_stop = EarlyStopping(monitor='val_loss',patience=10,verbose=1,
                           restore_best_weights=True)
history = imdb.fit(partial_X_train,partial_y_train,epochs=50,
                   batch_size=512,validation_data = (X_valid,y_valid),
                   callbacks=[reduce_Lr,early_stop])
