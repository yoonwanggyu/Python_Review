# 04.15-----------------------------------------------------------------------------------------------------------------------------------------------

    # 1) AND 연산 퍼셉트론 구현
        # 둘다 1 일땐 -> 1
        # 나머지는 다 -> 0
import numpy as np

def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5]) # 초기값
    b = -0.7                # 절편
    tmp = np.sum(w*x) + b   # y값 = sum((w^T * X) + b)

    if tmp <= 0:
        return 0
    else:
        return 1
    
data = []
for xs in [(0,0),(1,0),(0,1),(1,1)]:
    data.append(xs)
    print(data[0])
    y = AND(xs[0],xs[1])
    print(str(xs) + "->" + str(y))


    # 2) NAND 연산 퍼셉트론 구현
        # AND연산자 반대 
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp =  np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = NAND(xs[0], xs[1])
    print(str(xs) + " -> " + str(y))


    # 3) OR 연산자 퍼셉트론 구현
        # 둘중에 하나라도 1이면 -> 1
        # (0,0) -> 0
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([2,2])
    b = -1
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
    
for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = OR(xs[0],xs[1])
    print(str(xs) + "->" + str(y))


    # 4) XOR 연산 다층 퍼셉트론 구현
        # (0,0) -> 0
        # (0,1) -> 1
        # (1,0) -> 1
        # (1,1) -> 0
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y

print("NAND, OR, AND")
for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = XOR(xs[0], xs[1])
    print(str(xs) + " -> " + str(y))


    # 5) 텐서플로우 익히기
        # 자동 미분
import tensorflow as tf
x = tf.Variable(0.)  # 변수 x에 초기값으로 0.0을 할당하는 것
    # TensorFlow에서 변수를 사용할 때는 그 변수를 사용하기 전에 초기화해야 합니다. 이는 TensorFlow가 변수에 대한 메모리를 할당하고 해당 변수를 사용할 준비를 하기 위함입니다. 
    # TensorFlow의 tf.Variable() 함수를 사용하여 변수를 선언하고 초기화할 수 있습니다. 그렇기 때문에 코드에서 x를 초기화하는 것은 그저 TensorFlow에게 이 변수를 사용할 것이라고 알려주는 것과 같습니다.

with tf.GradientTape() as tape: 
    y = 2 * x +3
grad_of_y_wrt_x = tape.gradient(y,x)  # y를 x에 대해서 미분
print(grad_of_y_wrt_x)


    # 6) Sequential API : 순차적 모델
tf.random.set_seed(777)
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import mse

    # 데이터 준비하기
        # 1-1) XOR -> 활성화 함수 : linear -> 직선 하나로는 XOR를 맞출 수 없음
x = np.array([[0,0],[1,0],[0,1],[1,1]])
y = np.array([[0],[1],[1],[0]])

model = Sequential()
model.add(Dense(1,input_shape = (2,),activation = 'linear'))    # layers.Dense랑 같은 의미(명시적으로 layers를 쓸 수도 있음 : 가독성)
model.compile(optimizer = 'SGD',    # 확률적 경사하강법 : 손실 함수를 최소화하기 위해 모델의 가중치를 업데이트
              loss = mse,           # mse 손실함수 사용 : 평균 제곱 오차 -> 회귀 문제에서 주로 사용 -> 차이가 작을수록 좋음
              metrics = ['acc'])    # 모델 성능 지표 : 정확도 -> 분류 문제에서 주로 사용
model.fit(x,y,epochs=500)
model.summary()
model.get_weights()

    # 1-2) XOR -> 층 2개 -> 활성화 함수 : sigmoid 
tf.random.set_seed(777) # 시드를 설정 함.

# 데이터 준비하기
x = np.array([[0,0],[1,0],[0,1],[1,1]])
y = np.array([[0],[1],[1],[0]])

# XOR gate이기 때문에 y값은 0,1,1,0임

#모델 구성하기
model = Sequential()
model.add(Dense(32, input_shape = (2, ), activation = 'relu')) # 32개로 여러개로 쌓음.
model.add(Dense(1, activation='sigmoid')) # 이진분류 모델 = sigmoid -> 3개부터는 softmax 사용
                                          # 출력층에 존재하는 뉴런의 수

#모델 준비하기
model.compile(optimizer = 'adam',   # Adam : 각 가중치마다 개별적인 학습률 유지 / 각 가중치의 경사를 추적 / 지역 최솟값이 아닌 전역 최솟값을 탐색 / 각 반복 단계에서 편향을 보정하여 초기 반복에서 학습률을 너무 높게 설정하는 것을 막아줌
              loss = mse,
              metrics = ['acc'])    # list형태로 평가지표를 전달

#학습시키기
model.fit(x,y,epochs = 100)
# 시각화
from tensorflow.keras.utils import plot_model
plot_model(model, show_shapes=True)
plt.show()


    # 7) Activation Functions
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def relu(z):
    return np.maximum(0, z)

def derivative(f, z, eps=0.000001):
    return (f(z + eps) - f(z - eps))/(2 * eps)
import matplotlib.pyplot as plt

z = np.linspace(-5, 5, 200)

plt.figure(figsize=(11,4))

plt.subplot(121)
plt.plot(z, np.sign(z), "r-", linewidth=1, label="Step")
plt.plot(z, sigmoid(z), "g--", linewidth=2, label="Sigmoid")
plt.plot(z, np.tanh(z), "b-", linewidth=2, label="Tanh")
plt.plot(z, relu(z), "m-.", linewidth=2, label="ReLU")
plt.grid(True)
plt.legend(loc="center right", fontsize=14)
plt.title("Activation functions", fontsize=14)
plt.axis([-5, 5, -1.2, 1.2])

#plt.legend(loc="center right", fontsize=14)
plt.title("Derivatives", fontsize=14)
plt.axis([-5, 5, -0.2, 1.2])

plt.show()


    # 8) Functional API -> sequential 모델에 비해 복잡하고 다양한 유형의 연결을 다룰 때 사용 / 다중 입력(출력) 모델 
## data 선언
x_data = np.array([[0.0,0.0], [0.0,1.0], [1.0,0.0],[1.0,1.0]])
y_data = np.array([[0], [1], [1], [0]])

    # 입력층
input_layer = tf.keras.layers.Input(shape=(2,))
    # 은닉층
x = tf.keras.layers.Dense(2,activation='sigmoid')(input_layer)
    # 출력층
Out_layer = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    # 모델 정의
model = tf.keras.Model(inputs=[input_layer],outputs=[Out_layer])
    # 모델 확인
model.summary()

optimizer = tf.keras.optimizers.SGD(learning_rate=0.7)
loss = tf.keras.losses.binary_crossentropy              # 이진분류 문제에 사용 : 실제값과 예측값 사이의 교차 엔트로피 계산
metrics = tf.keras.metrics.binary_accuracy              # 예측한 이진 레이블이 실제 레이블과 일치하는 비율 계산

model.compile(loss = loss, optimizer = optimizer, metrics = [metrics])

model.fit(x_data,y_data,epochs=100, batch_size=4)

    # custom model
input_A = tf.keras.layers.Input(shape=[5],name='wide_input')
input_B = tf.keras.layers.Input(shape=[6],name='deep_input')
hidden1 = tf.keras.layers.Dense(30,activation='relu')(input_B)
hidden2 = tf.keras.layers.Dense(30,activation='relu')(hidden1)
concat = tf.keras.layers.concatenate([input_A,hidden2])
output = tf.keras.layers.Dense(1,name='output')(concat)     # activation기본값은 Linear
model = tf.keras.Model(inputs=[input_A,input_B],outputs=[output])
model.summary()


    # 9) Class 수업
        # class : 객체를 만드는 구조/틀
        # instance : 클래스가 실질적으로 객체를 만들었을 때, 그 객체를 부르는 용어
class CustomNumers:
    def __init__(self):
        self._numbers = [n for n in range(1,11)]

a = CustomNumers()
a[2:5]      # 불가
a._numbers[2:5]

class CustomNumers:
    def __init__(self):
        self._numbers = [n for n in range(1,11)]
    
    def __getitem__(self,idx):
        return self._numbers[idx]
    
a = CustomNumers()
a[2:5]      # 가능 -> __getitem__ 때문

    # 10) MPG 데이터로 실습 -> colab으로 실습
https://colab.research.google.com/drive/1g1azaxxvabXN9PzgNqJdWgGb1DcwF5KM?usp=sharing


# 04.16-----------------------------------------------------------------------------------------------------------##

    # 선형 회귀를 기반 심층 신경망 예제 코드를 작성하고 실습해보자---------
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

x_train = np.array([1,2,3,4,5])
y_train = np.array([2,4,6,8,10])

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(1,)),
    tf.keras.layers.Dense(5,activation='relu')
])

model.add(tf.keras.layers.Dense(3,activation="relu"))
model.add(tf.keras.layers.Dense(1))

model.summary()

optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)


    # MNIST 데이터 셋 -----------
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

X_train.shape
y_train.shape
X_test.shape
y_test.shape

        # 데이터 그림으로 확인
sample_size = 10
random_idx = np.random.randint(60000,size=sample_size)
for idx in random_idx:
    img = X_train[idx, :]
    label = y_train[idx]
    plt.figure()
    plt.imshow(img)
    plt.title(f"{idx}-th data, label is {label}")
    plt.show()

        # X_train에서 검증 데이터 만들기
from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.3,random_state=777)
print(f"훈련 데이터 {X_train.shape} 레이블 {y_train.shape}")
print(f"검증 데이터 {X_val.shape} 레이블 {y_val.shape}")

        # 모델 입력을 위해 데이터를 784차원으로 변경
num_X_train = X_train.shape[0]
num_X_val = X_val.shape[0]
num_X_test = X_test.shape[0]

X_train = (X_train.reshape((num_X_train,28*28))) / 255
X_val = (X_val.reshape((num_X_val,28*28))) / 255
X_test = (X_test.reshape((num_X_test,28*28))) / 255
X_train.shape
y_train     # 답

        # 원핫인코딩 -> 답인것만 1로 표시 -> 모델은 몇번째가 답인지 모름
from tensorflow.keras.utils import to_categorical
num_classes = 10
y_train = to_categorical(y_train,num_classes)
y_val = to_categorical(y_val,num_classes)
y_test = to_categorical(y_test,num_classes)
y_train

        # 모델 구성하기
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense # type: ignore

model = Sequential([
    tf.keras.layers.Input(shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu')
])
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))
model.summary()

        # 학습과정 설정하기
model.compile(optimizer='adam',
              loss="categorical_crossentropy",      # 위에서 답을 원핫인코딩 안했을 경우 sparse_를 붙여서 쓰기(sparse_categorical_crossentropy)
              metrics=['acc'])

        # 모델 학습하기
history = model.fit(X_train,y_train,
                    epochs=30,
                    batch_size=128,
                    validation_data=(X_val,y_val),
                    verbose = 1)

        # 결과 시각화
def plot_history(history):
    plt.figure(figsize=(10,5))

    epochs = range(1,len(history.history['loss'])+1)

    plt.subplot(1,2,1)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.plot(epochs,history.history['loss'],label="train_loss")
    plt.plot(epochs,history.history['val_loss'],label="val_loss")
    plt.legend()
    plt.title("train and val loss")

    plt.subplot(1,2,2)
    plt.xlabel("Epochs")
    plt.ylabel("Acc")
    plt.plot(epochs,history.history['acc'],label="train_acc")
    plt.plot(epochs,history.history['val_acc'],label="val_acc")
    plt.legend()
    plt.title("train and val acc")

    plt.show()

plot_history(history)

        # 모델 평가
model.evaluate(X_test,y_test)

results = model.predict(X_test)
results[0]      # 10개에 클래스에 대한 확률 분포를 출력
results.shape
np.set_printoptions(precision=7)    # 소숫점 7자리
print(f"각 클래스에 속할 확률 : {results[0]}")

arg_results = np.argmax(results,axis=-1)    # 10개의 클래스에 대한 확률 분포중 가장 높은 값 출력 / axis=-1은 배열의 마지막 차원(여기서는 클래스에 해당하는 차원)을 따라 최대값을 찾도록 지정
plt.imshow(X_test[0].reshape(28,28))
plt.title("Predicted value of the first image :" + str(arg_results[0]))
plt.show()

        # 모델 평가 -> 혼동행렬
from sklearn.metrics import classification_report,confusion_matrix
import seaborn as sns

plt.figure(figsize=(7,7))
cm = confusion_matrix(np.argmax(y_test,axis=-1),np.argmax(results,axis=-1))
sns.heatmap(cm,annot=True,fmt='d',cmap='Blues')
plt.xlabel('predicted label')
plt.ylabel('true label')
plt.show()

        # 모델 평가 -> 분류 보고서
print('\n', classification_report(np.argmax(y_test, axis = -1), np.argmax(results, axis = -1)))


    # Fashion MNIST------------------------------------------
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images.shape
test_images.shape
train_labels

img = train_images[0,:]
label = train_labels[0]
plt.imshow(img)
plt.title(f"첫번째 사진: {label}")
plt.show()

train_images = train_images / 255.0
test_images = test_images / 255.0

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])  # 눈금 제거
    plt.yticks([])
    plt.grid(False) # 그리드 제거
    plt.imshow(train_images[i],cmap=plt.cm.binary) # cmap=plt.cm.binary는 흑백 이미지로
    plt.xlabel(class_names[train_labels[i]])
    plt.show()

from tensorflow.keras.layers import Flatten

model = Sequential([
    Flatten(input_shape = (28,28)),
    Dense(64,activation='relu'),
    Dense(32,activation='relu'),
    Dense(10,activation='softmax')
])

model.compile(optimizer='adam',
              loss = "sparse_categorical_crossentropy",
              metrics = ['acc'])

model.fit(train_images,train_labels,epochs=10,validation_split=0.2)

predictions = model.predict(test_images)
predictions[0]
np.argmax(predictions[0])
test_labels[0]

            # 결과 시각화
def plot_image(i, predictions_array, true_label, img):
  #image내에 array : pixel을 가져 오는 것 img : 실제 이미지이고, predictions_array->예측 한것 (데이터로 부터)
  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary) #gray_scale

  predicted_label = np.argmax(predictions_array)
  #predictions_array->예측 한것 (데이터로 부터)->최대인 label을 뽑아주세요.
  if predicted_label == true_label: #예측한 라벨 == 실제 라벨
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777") #hex color : 색의 혼합.
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

i = 300
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions, test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions,  test_labels)
plt.show()

    # IMDB 이진 분류----------------------------------------------------
from keras.datasets import imdb
from keras.callbacks import ReduceLROnPlateau,EarlyStopping
from keras.layers import BatchNormalization,Dropout
from keras.regularizers import l2

(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=10000)
max([max(sequence) for sequence in train_data])

        # 원핫인코딩
def vectorize_sequences(sequences,dimension=10000):
    result = np.zeros((len(sequences),dimension))     # 모든 요소가 0인 배열 생성(밑그림) / len(sequences)는 배열의 행의 개수 / dimension은 배열의 열의 개수
    for i,sequences in enumerate(sequences):
        result[i,sequences] = 1.0
    return result

        # 데이터 원핫인코딩
X_train = vectorize_sequences(train_data)
X_test = vectorize_sequences(test_data)
y_train = np.asarray(train_labels).astype(float)
y_test = np.asarray(test_labels).astype(float)

        # 모델 생성
imdb = Sequential()
imdb.add(Dense(32,input_shape=(10000,),activation='relu', kernel_initializer='he_normal',kernel_regularizer=l2(0.001)))
# imdb.add(BatchNormalization())
# imdb.add(Dropout(0.2))
# imdb.add(Dense(32,activation='relu',kernel_regularizer=l2(0.001)))
imdb.add(Dense(1,activation="sigmoid"))

imdb.compile(loss = "binary_crossentropy",
             optimizer='adam',
             metrics=['acc'])
        # 모델 훈련
X_valid = X_train[:10000]
partial_X_train = X_train[10000:]
y_valid = y_train[:10000]
partial_y_train = y_train[10000:]

            # 학습률을 감소시키는 콜백
                # monitor : 검사할 지표
                # factor: 학습률을 감소시킬 비율을 지정 / 기본값은 0.1
                # patience: 지정된 지표(monitor)가 개선되지 않은 에포크의 수를 지정 / 이 수만큼 지표가 개선되지 않으면 학습률이 감소됩니다.
                # min_lr: 학습률의 하한입니다. 학습률은 이 값보다 낮아지지 않습니다.
reduce_Lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,
                              min_lr=0.0001)

            # 모니터링된 지표가 개선되지 않을 때 트레이닝을 중지
                # patience: 검증 손실이 개선되지 않는 에포크 수 /  예를 들어, 10으로 설정하면 검증 손실이 10회 연속 개선되지 않으면 트레이닝이 중지됩니다.
                # verbose: 상세 모드입니다. 1로 설정하면 조기 중지로 트레이닝이 중지될 때 메시지를 출력합니다.
                # restore_best_weights: 모델의 가중치를 모니터링된 지표의 최상의 값을 가진 에포크로 복원할지 여부입니다. True로 설정하면 모델 가중치가 검증 세트에서 최상의 성능을 보인 에포크의 값으로 설정됩니다.
early_stop = EarlyStopping(monitor='val_loss',patience=10,verbose=1,
                           restore_best_weights=True)
history = imdb.fit(partial_X_train,partial_y_train,epochs=50,
                   batch_size=512,validation_data = (X_valid,y_valid),
                   callbacks=[reduce_Lr,early_stop])


# 04.17-----------------------------------------------------------------------------------------------------------##

    # 영어 Word2Vec 만들기

import re
import urllib.request
import zipfile
from lxml import etree
from nltk.tokenize import word_tokenize, sent_tokenize
import nltk
nltk.download('punkt')

    # 훈련 데이터 다운로드
    # 데이터 다운로드
urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml", filename="ted_en-20160408.xml")

targetXML = open("ted_en-20160408.xml","r",encoding='UTF8')
target_text = etree.parse(targetXML)

parse_text = '\n'.join(target_text.xpath('//content/text()'))

content_text = re.sub(r'\([^)]*\)',"",parse_text)

sent_text = sent_tokenize(content_text)

normalized_text=[]
for string in sent_text:
    tokens = re.sub(r"[^a-z0-9]+"," ",string.lower())
    normalized_text.append(tokens)

result = [word_tokenize(sentence) for sentence in normalized_text]
len(result)

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

model = Word2Vec(sentences=result,
                 vector_size=100,
                 window=5,      # 앞뒤로 몇개 자를지
                 min_count=5,
                 workers=4,
                 sg=0)

model_result = model.wv.most_similar("man")


    # 20 뉴스그룹 데이터 전처리하기
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from sklearn.datasets import fetch_20newsgroups
from tensorflow.keras.preprocessing.text import Tokenizer

dataset = fetch_20newsgroups(shuffle = True, random_state=1,remove=("headers","footers","quotes"))
documents = dataset.data

news_df = pd.DataFrame({'documents' : documents})
news_df['clean_doc'] = news_df['documents'].str.replace("[^a-zA-Z]"," ")
news_df['clean_doc'] = news_df['documents'].apply(lambda x : ' '.join([w for w in x.split() if len(w)>3]))
news_df['clean_doc'] = news_df['documents'].apply(lambda x : x.lower())

news_df.isnull().sum().any()

news_df.replace("",float("NaN"),inplace=True)
news_df.isnull().sum().any()
news_df.dropna(inplace=True)

import nltk
nltk.download('stopwords')
stopwords = stopwords.words('english')
tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())
tokenized_doc = tokenized_doc.apply(lambda x : [item for item in x if item not in stopwords])
tokenized_doc = tokenized_doc.to_list()

drop_train = [index for index,sentence in enumerate(tokenized_doc) if len(sentence)<=1]
tokenized_doc = np.delete(np.array(tokenized_doc,dtype=object),drop_train,axis=0)
len(tokenized_doc)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(tokenized_doc)

word2idx = tokenizer.word_index
idx2word = {value : key for key,value in word2idx.items()}
encoded = tokenizer.texts_to_sequences(tokenized_doc)
encoded[:2]
len(word2idx) + 1

#단어 집합의 크기 확인
vocab_size = len(word2idx) + 1
print('단어 집합의 크기 :', vocab_size)

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Reshape, Activation, Input
from tensorflow.keras.layers import Dot
from tensorflow.keras.utils import plot_model
from IPython.display import SVG

embedding_dim = 100

# 중심 단어를 위한 임베딩 테이블
w_inputs = Input(shape=(1, ), dtype='int32')
word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)

# 주변 단어를 위한 임베딩 테이블
c_inputs = Input(shape=(1, ), dtype='int32')
context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)

dot_product = Dot(axes=2)([word_embedding, context_embedding])
dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)
output = Activation('sigmoid')(dot_product)

model = Model(inputs=[w_inputs, c_inputs], outputs=output)
model.summary()
model.compile(loss='binary_crossentropy', optimizer='adam')
plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')

for epoch in range(1, 6):
    loss = 0
    for _, elem in enumerate(skip_grams):
        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')
        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')
        labels = np.array(elem[1], dtype='int32')
        X = [first_elem, second_elem]
        Y = labels
        loss += model.train_on_batch(X,Y)
    print('Epoch :',epoch, 'Loss :',loss)

import gensim

f = open('vectors.txt' ,'w')
f.write('{} {}\n'.format(vocab_size-1, embedding_dim))
vectors = model.get_weights()[0]
for word, i in tokenizer.word_index.items():
    f.write('{} {}\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))
f.close()

# 모델 로드
w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)

w2v.most_similar(positive=['soldiers'])


# 04.18-----------------------------------------------------------------------------------------------------------##
    
	# RNN 실습-----------------------------##
import tensorflow as tf
print(tf.__version__)

import numpy as np
a = np.arange(10)
np.where(a<5,a,10*a)

import matplotlib.pyplot as plt

        # 데이터
np.random.seed(2020)
time = np.arange(30 * 12 +1)
month_time = (time % 30) / 30
time_series = 20 * np.where(month_time < 0.5,
                            np.cos(2*np.pi*month_time),
                            np.cos(2*np.pi*month_time)+np.random.random(361))

plt.figure(figsize = (10, 5))
plt.title('TimeSeries Data')
plt.xlabel('Time')
plt.ylabel('Value')
plt.plot(np.arange(0, 30 * 11 + 1), time_series[:30 * 11 + 1], color = 'black', alpha = 0.7, label = 'train') # 학습용 데이터
plt.plot(np.arange(30 * 11, 30 * 12 + 1), time_series[30 * 11:], color = 'orange', label = 'test') # 테스트용 데이터
plt.legend()
plt.show()

        # 데이터 전처리
def make_sequence(time_series, n):
    x_train, y_train = list(), list()

    for i in range(len(time_series)):
        x = time_series[i:(i + n)]
        if (i + n) < len(time_series):
            x_train.append(x)
            y_train.append(time_series[i + n])
        else:
            break

    return np.array(x_train), np.array(y_train)

n = 10
x_train, y_train = make_sequence(time_series, n)

x_train = x_train.reshape(-1, n, 1)
y_train = y_train.reshape(-1, 1)

from sklearn.model_selection import train_test_split

patial_x_train = x_train[:30 * 11]
patial_y_train = y_train[:30 * 11]
x_test = x_train[30 * 11:]
y_test = y_train[30 * 11:]

print('train:', patial_x_train.shape, patial_y_train.shape)
print('test:',x_test.shape, y_test.shape)

        # 데이터 형태 확인
test_arr = np.arange(100)
a, b = make_sequence(test_arr, 10)

for i in range(1, 4):
    print(a[i],'|', b[i])

        # simple RNN 사용
from tensorflow.keras.layers import SimpleRNN,Flatten,Dense,Conv1D
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(SimpleRNN(units=32,activation='tanh',input_shape=(n,1)))
model.add(Dense(1,activation='linear'))
model.compile(optimizer='adam',
              loss="mse")
model.summary()
model.fit(x_train,y_train,epochs=200,batch_size=64)

        # 예측 결과 그려보기
pred = model.predict(x_test)
pred_range = np.arange(len(y_train),len(y_train)+len(pred))
plt.figure(figsize=(15,5))
plt.title('Prediction')
plt.xlabel('Time'); plt.ylabel('Value')
plt.plot(pred_range, y_test.reshape(-1,), color='orange', label='ground-truth')
plt.plot(pred_range, pred.reshape(-1,), color='blue', label='prediction')
plt.legend()
plt.show()



    # Embedding층을 사용해 단어 임베딩 학습하기-----------------##
from tensorflow.keras.layers import Embedding
# 가능한 토큰의 개수(여기서는 1,000으로 단어 인덱스 최댓값 + 1입니다)와 임베딩 차원(여기서는 64)입니다
embedding_layer = Embedding(1000,64)


    # IMDB 데이터 활용-----------SimpleRNN------------------------------------------------##
from keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Embedding
            # num_words=10000은 이 데이터셋에서 사용할 단어의 최대 개수를 지정하는 인자
(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=10000)
            # pad_sequences 함수는 시퀀스 데이터의 길이를 특정 길이로 맞추기 위해 사용
            # 만약 시퀀스의 길이가 500보다 짧다면 빈 부분은 0으로 패딩됩니다. 
            # 만약 시퀀스의 길이가 500을 초과한다면 길이가 500으로 자르게 됩니다.
pad_X_train = pad_sequences(X_train,maxlen=500)     # 최대길이를 맞춰줌
pad_X_test = pad_sequences(X_test,maxlen=500)

model = Sequential()
model.add(Embedding(input_dim=10000,output_dim=32)) # 원핫인코딩을 하지 않고 Embedding 함수로 자동처리
            # return_sequences = True : 모든 시점의 은닉 상태를 출력
            # recurrent_sequences = True : 순환되는 은닉 상태에 드롭아웃을 적용
            # 함께 사용하면, 모든 시점의 은닉 상태를 출력하고 동시에 순환 상태에 드롭아웃을 적용하여 모델의 효율성을 향상시킬 수 있습니다.
model.add(SimpleRNN(32,return_sequences=True,dropout=0.15,recurrent_dropout=0.15))
model.add(SimpleRNN(32))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(pad_X_train,y_train,
                    batch_size=32,
                    epochs=10,
                    validation_split=0.2)

his_dict = history.history
loss = his_dict['loss']
val_loss = his_dict['val_loss'] # 검증 데이터가 있는 경우 val_ 수식어가 붙는다.
epochs = range(1,len(loss)+1 )
fig = plt.figure(figsize= (10,5) )

        # 학습 및 검증 손실 그리기
ax1 = fig.add_subplot(1,2,1)
ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')
ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')
ax1.set_title('train and val loss')
ax1.set_xlabel('epochs')
ax1.set_ylabel('loss')
ax1.legend()
        # 학습 및 검증 정확도 그리기
acc = his_dict['acc']
val_acc = his_dict['val_acc']
ax2 = fig.add_subplot(1,2,2)
ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')
ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')
ax2.set_title('train and val acc')
ax2.set_xlabel('epochs')
ax2.set_ylabel('acc')
ax2.legend()

plt.show() 


    # 스펨 메일 분류하기------------------------RNN----------------------------------##
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv", filename="spam.csv")
data = pd.read_csv('spam.csv', encoding='latin1')
data[:5]
del data['Unnamed: 2']
del data['Unnamed: 3']
del data['Unnamed: 4']

            # ham : 정상(0) / spham : 비정상(1)
data['v1'] = data['v1'].replace(['ham','spam'],[0,1])
data

data.info()
data.isnull().sum()
data['v1'].value_counts()
data['v1'].value_counts().plot(kind='bar')
plt.show()

X_data = data['v2']
y_data = data['v1']
X_train,X_test,y_train,y_test = train_test_split(X_data,y_data,test_size=0.2,random_state=42,stratify=y_data)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train_encoded = tokenizer.texts_to_sequences(X_train)
X_train_encoded[:5]
word_to_index = tokenizer.word_index
word_to_index

            #빈도수가 낮은 단어들이 훈련 데이터에서 얼만큼의 비중을 차지하는지 확인
threshold = 2
total_cnt = len(word_to_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

for key,value in tokenizer.word_counts.items():
    total_freq += value
    if (value < threshold):
        rare_cnt += 1
        rare_freq += value
    
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합(vocabulary)에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)

vocab_size = len(word_to_index)+1
vocab_size
print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))
print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))
plt.hist([len(sample) for sample in X_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

            # 길이 맞추기
X_train_padded = pad_sequences(X_train_encoded,maxlen=189)
X_train_padded.shape

from tensorflow.keras.layers import SimpleRNN, Embedding, Dense
from tensorflow.keras.models import Sequential
    
model = Sequential()
model.add(Embedding(vocab_size,32))
model.add(SimpleRNN(32))
model.add(Dense(1,activation='sigmoid'))
model.summary()
model.compile(optimizer='adam',
              loss="binary_crossentropy",
              metrics = ['acc'])
history = model.fit(X_train_padded,y_train,
                    epochs=5,
                    batch_size=64,
                    validation_split=0.2)

            # 테스트
X_test_encoded = tokenizer.texts_to_sequences(X_test)
X_test_padded = pad_sequences(X_test_encoded,maxlen=189)
model.evaluate(X_test_padded,y_test)

epochs = range(1, len(history.history['loss']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()


# IMDB 데이터 활용-----------LSTM------------------------------------------------##
from tensorflow.keras.datasets import imdb

(X_train,y_train), (X_test,y_test) = imdb.load_data()
len(X_train)
len(X_test)
X_test[0]
        # X-train데이터 안에 있는 리스트들의 길이 제기
len_result = [len(s) for s in X_train]
        # 리스트들의 길이 중 가장 큰 길이 확인
np.max(len_result)
        # 리스트들의 길이 평균 확인
np.mean(len_result)

        # 모델 만들기
                # 위에서 max와 mean보고 임의로 5000개까지만
(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=5000)
X_train_padded = pad_sequences(X_train,maxlen=500)
X_test_padded = pad_sequences(X_test,maxlen=500)

from tensorflow.keras.layers import LSTM
model = Sequential()
model.add(Embedding(5000,120))
model.add(LSTM(120))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'])
histoty = model.fit(X_train_padded,y_train,
                    batch_size=64,
                    epochs=15,
                    validation_split=0.2)
model.evaluate(X_test_padded,y_test)

hist_dict = history.history
hist_dict.keys()

loss=hist_dict['loss']
val_loss=hist_dict['val_loss']
epochs=range(1, len(hist_dict['loss'])+1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()
plt.plot(epochs, hist_dict['acc'], 'bo', label='Training acc')
plt.plot(epochs, hist_dict['val_acc'], 'b', label= 'valadation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
